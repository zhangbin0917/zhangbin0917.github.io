<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[遥感数据集]]></title>
    <url>%2F2018%2F06%2F12%2F%E9%81%A5%E6%84%9F%E6%95%B0%E6%8D%AE%E9%9B%86%2F</url>
    <content type="text"><![CDATA[整理了遥感中的数据集，长期更新！！ 场景分类1. UC Merced Land Use Datasethttp://vision.ucmerced.edu/datasets/landuse.htmlThis is a 21 class land use image dataset meant for research purposes.There are 100 images for each of the following classes:agricultural，airplane，baseballdiamond，beach，buildings，chaparral，denseresidential，forest，freeway，golfcourse，harbor，intersection，mediumresidential，mobilehomepark，overpass，parkinglot，river，runway，sparseresidential，storagetanks，tenniscourtEach image measures 256x256 pixels.The images were manually extracted from large images from the USGS National Map Urban Area Imagery collection for various urban areas around the country. The pixel resolution of this public domain imagery is 1 foot.Reference： Yi Yang and Shawn Newsam, “Bag-Of-Visual-Words and Spatial Extensions for Land-Use Classification,” ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems (ACM GIS), 2010. 2. WHU-RS19http://captain.whu.edu.cn/repository.htmlhttp://captain.whu.edu.cn/datasets/WHU-RS19.zipWHU-RS19是从谷歌卫星影像上获取19类遥感影像，可用于场景分类和检索。Reference： G.-S. Xia, W. Yang, J. Delon, Y. Gousseau. H. Maitre, H. Sun, “Structural high-resolution satellite image indexing”. Symposium: 100 Years ISPRS - Advancing Remote Sensing Science: Vienna, Austria, 2010 3. RSSCN7https://sites.google.com/site/qinzoucn/documentsThis dataset contains 2800 remote sensing images which are from 7 typical scene categories - the grass land, forest, farm land, parking lot, residential region, industrial region, and river&amp;lake. For each category, there are 400 images collected from the Google Earth which are sampled on 4 different scales with 100 images per scale. Each image has a size of 400*400 pixels. This dataset is rather challenging due to the wide diversity of the scene images which are captured under changing seasons and varying weathers, and sampled with different scales.Reference： Qin Zou, Lihao Ni, Tong Zhang and Qian Wang, Deep learning based feature selection for remote sensing scene classification, IEEE Geoscience and Remote Sensing Letters, vol. 12, no. 11, pp.2321-2325, 2015. 4. SAT-4 and SAT-6 airborne datasetshttp://csc.lsu.edu/~saikat/deepsat/https://arxiv.org/abs/1509.03602Images were extracted from the National Agriculture Imagery Program (NAIP) dataset. The NAIP dataset consists of a total of 330,000 scenes spanning the whole of the Continental United States (CONUS). We used the uncompressed digital Ortho quarter quad tiles (DOQQs) which are GeoTIFF images and the area corresponds to the United States Geological Survey (USGS) topographic quadrangles. The average image tiles are ~6000 pixels in width and ~7000 pixels in height, measuring around 200 megabytes each. The entire NAIP dataset for CONUS is ~65 terabytes. The imagery is acquired at a 1-m ground sample distance (GSD) with a horizontal accuracy that lies within six meters of photo-identifiable ground control points. The images consist of 4 bands - red, green, blue and Near Infrared (NIR). In order to maintain the high variance inherent in the entire NAIP dataset, we sample image patches from a multitude of scenes (a total of 1500 image tiles) covering different landscapes like rural areas, urban areas, densely forested, mountainous terrain, small to large water bodies, agricultural areas, etc. covering the whole state of California. An image labeling tool developed as part of this study was used to manually label uniform image patches belonging to a particular landcover class. Once labeled, 28x28 non-overlapping sliding window blocks were extracted from the uniform image patch and saved to the dataset with the corresponding label. We chose 28x28 as the window size to maintain a significantly bigger context, and at the same time not to make it as big as to drop the relative statistical properties of the target class conditional distributions within the contextual window. Care was taken to avoid interclass overlaps within a selected and labeled image patch.The datasets are encoded as MATLAB .mat files that can be read using the standard load command in MATLAB. Each sample image is 28x28 pixels and consists of 4 bands - red, green, blue and near infrared. The training and test labels are 1x4 and 1x6 vectors for SAT-4 and SAT-6 respectively having a single 1 indexing a particular class from 0 through 4 or 6 and 0 values at all other indices.Reference: Saikat Basu, Sangram Ganguly, Supratik Mukhopadhyay, Robert Dibiano, Manohar Karki and Ramakrishna Nemani, DeepSat - A Learning framework for Satellite Imagery, ACM SIGSPATIAL 2015. 5. RSC11https://www.researchgate.net/publication/271647282_RS_C11_Database611 scenes, all using high resolution remote sensing images, downloaded from Google Earth 6. SIRI-WHUhttp://www.lmars.whu.edu.cn/prof_web/zhongyanfei/e-code.htmlThis is a 12-class Google image dataset of SIRI-WHU meant for research purposes.There are 200 images for each of the following classes:Agriculture, Commercial, Harbor, Idle land, Industrial, Meadow, Overpass, Park, Pond, Residential, River, WaterEach image measures 200*200 pixels, with a 2-m spatial resolution.This dataset was acquired from Google Earth (Google Inc.) and mainly covers urban areas in China, and the scene image dataset is designed by RS_IDEA Group in Wuhan University (SIRI-WHU).Reference: B. Zhao, Y. Zhong, G.-s. Xia, and L. Zhang, “Dirichlet-Derived Multiple Topic Scene Classification Model Fusing Heterogeneous Features for High Spatial Resolution Remote Sensing Imagery,” IEEE Transactions on Geoscience and Remote Sensing, vol. 54, no. 4, pp. 2108-2123, Apr. 2016. B. Zhao, Y. Zhong, L. Zhang, and B. Huang, “The Fisher Kernel Coding Framework for High Spatial Resolution Scene Classification,” Remote Sensing, vol. 8, no. 2, p. 157, doi:10.3390/rs8020157 2016. Q. Zhu, Y. Zhong, B. Zhao, G.-S. Xia, and L. Zhang, “Bag-of-Visual-Words Scene Classifier with Local and Global Features for High Spatial Resolution Remote Sensing Imagery,” IEEE Geoscience and Remote Sensing Letters, DOI:10.1109/LGRS.2015.2513443 2016. 7. AIDhttp://www.lmars.whu.edu.cn/xia/AID-project.htmlhttps://captain-whu.github.io/AIDAID is a new large-scale aerial image dataset, by collecting sample images from Google Earth imagery. Note that although the Google Earth images are post-processed using RGB renderings from the original optical aerial images, it has proven that there is no significant difference between the Google Earth images with the real optical aerial images even in the pixel-level land use/cover mapping. Thus, the Google Earth images can also be used as aerial images for evaluating scene classification algorithms.The new dataset is made up of the following 30 aerial scene types: Aairport, bare land, baseball field, beach, bridge, center, church, commercial, dense residential, desert, farmland, forest, industrial, meadow, medium residential, mountain, park, parking, playground, pond, port, railway station, resort, river, school, sparse residential, square, stadium, storage tanks and viaduct. All the images are labelled by the specialists in the field of remote sensing image interpretation, and some samples of each class are shown in Fig. In all, the AID dataset has a number of 10000 images within 30 classes.Reference: G.-S. Xia, J. Hu, F. Hu, B. Shi, X. Bai, Y. Zhong, L. Zhang, X. Lu, “AID: A benchmark dataset for performance evaluation of aerial scene classification”, IEEE Transactions on Geoscience and Remote Sensing, vol. 55, no. 7, pp. 3965-3981, 2017. 8. NWPU-RESISC45http://www.escience.cn/people/JunweiHan/NWPU-RESISC45.htmlhttps://arxiv.org/abs/1703.00121NWPU-RESISC45 dataset is a publicly available benchmark for REmote Sensing Image Scene Classification (RESISC), created by Northwestern Polytechnical University (NWPU). This dataset contains 31,500 images, covering 45 scene classes with 700 images in each class. These 45 scene classes include airplane, airport, baseball diamond, basketball court, beach, bridge, chaparral, church, circular farmland, cloud, commercial area, dense residential, desert, forest, freeway, golf course, ground track field, harbor, industrial area, intersection, island, lake, meadow, medium residential, mobile home park, mountain, overpass, palace, parking lot, railway, railway station, rectangular farmland, river, roundabout, runway, seaice, ship, snowberg, sparse residential, stadium, storage tank, tennis court, terrace, thermal power station, and wetland.Reference: G. Cheng, J. Han, X. Lu. Remote Sensing Image Scene Classification: Benchmark and State of the Art. Proceedings of the IEEE. 9. PatternNethttps://sites.google.com/view/zhouwx/datasetPatternNet is a large-scale high-resolution remote sensing dataset collected for remote sensing image retrieval. There are 38 classes and each class has 800 images of size 256×256 pixels. The images in PatternNet are collected from Google Earth imagery or via the Google Map API for some US cities. The figure shows some example images from each class.Reference: Zhou, W., Newsam, S., Li, C., &amp; Shao, Z. (2017). PatternNet: A Benchmark Dataset for Performance Evaluation of Remote Sensing Image Retrieval. arXiv preprint arXiv:1706.03424. Zhou, W., Newsam, S., Li, C., &amp; Shao, Z. (2017). Learning Low Dimensional Convolutional Neural Networks for High-Resolution Remote Sensing Image Retrieval. Remote Sensing, 9(5), 489. Zhou, W., Shao, Z., Diao, C., &amp; Cheng, Q. (2015). High-resolution remote-sensing imagery retrieval using sparse features by auto-encoder. Remote Sensing Letters, 6(10), 775-783. 10. RSI-CBhttps://arxiv.org/abs/1705.10450https://github.com/lehaifeng/RSI-CBConsidering the different image size requirements of the DCNN, construct two datasets of 256 × 256 and 128 × 128 pixel sizes (RSI-CB256 and RSI-CB128, respectively) with 0.3–3-m spatial resolutions. The former contains 35 categories and more than 24,000 images. The latter contains 45 categories and more than 36,000 images. We establish a strict object category system according to the national standard of land-use classification in China and the hierarchical grading mechanism of ImageNet. The six categories are agricultural land, construction land and facilities, transportation and facilities, water and water conservancy facilities, woodland, and other land. 11. AID++https://arxiv.org/abs/1806.00801Reference: AID++：An Updated Version of AID on Scene Classification 目标检测1. RSOD-Datasethttps://github.com/RSIA-LIESMARS-WHU/RSOD-Dataset-It is an open dataset for object detection in remote sensing images. The dataset includes aircraft, oiltank, playground and overpass.The format of this dataset that for PASCAL VOC.The datase includes 4 files, and each file is for one kind of object. Please download the dataset files from BaiduYun. aircraft dataset, 4993 aircrafts in 446 images. playground, 191 playgrounds in 189 images. overpass, 180 overpass in 176 overpass. oiltank, 1586 oiltanks in 165 images. Reference: Y. Long, Y. Gong, Z. Xiao and Q. Liu, “Accurate Object Localization in Remote Sensing Images Based on Convolutional Neural Networks,” in IEEE Transactions on Geoscience and Remote Sensing, vol. 55, no. 5, pp. 2486-2498, May 2017. doi: 10.1109/TGRS.2016.2645610, link Z Xiao, Q Liu, G Tang, X Zhai, “Elliptic Fourier transformation-based histograms of oriented gradients for rotationally invariant object detection in remote-sensing images”, International Journal of Remote Sensing, vol. 36, no. 2, 2015 2. NWPU VHR-10 datasetNWPU VHR-10 dataset is a publicly available 10-class geospatial object detection dataset used for research purposes only.These ten classes of objects are airplane, ship, storage tank, baseballdiamond, tennis court, basketball court, ground track field, harbor, bridge,and vehicle. This dataset contains totally 800 very-high-resolution (VHR)remote sensing images that were cropped from Google Earth and Vaihingen dataset and then manually annotated by experts.Thisdataset can be downloaded from OneDrive or BaiduWangpan.Reference: G. Cheng, J. Han, P. Zhou, L. Guo. Multi-class geospatial object detection and geographic imageclassification based on collection of part detectors. ISPRS Journal ofPhotogrammetry and Remote Sensing, 98: 119-132, 2014. G. Cheng, J. Han. A survey on objectdetection in optical remote sensing images. ISPRS Journal of Photogrammetry andRemote Sensing, 117: 11-28, 2016. G. Cheng, P. Zhou, J. Han. Learningrotation-invariant convolutional neural networks for object detection in VHRoptical remote sensing images. IEEE Transactions on Geoscience and RemoteSensing, 54(12): 7405-7415, 2016. 3. Cars Overhead With Context(COWC)https://gdo152.llnl.gov/cowc/Poster PaperGithub: https://github.com/LLNL/cowcFTP: ftp://gdo152.ucllnl.org/cowc/The Cars Overhead With Context (COWC) data set is a large set of annotated cars from overhead. It is useful for training a device such as a deep neural network to learn to detect and/or count cars. The dataset has the following attributes: (1) Data from overhead at 15 cm per pixel resolution at ground (all data is EO). (2) Data from six distinct locations: Toronto Canada, Selwyn New Zealand, Potsdam and Vaihingen Germany, Columbus and Utah United States. (3) 32,716 unique annotated cars. 58,247 unique negative examples. (4) Intentional selection of hard negative examples. (5) Established baseline for detection and counting tasks. (6) Extra testing scenes for use after validation. Reference: Mundhenk T N, Konjevod G, Sakla W A, et al. A large contextual dataset for classification, detection and counting of cars with deep learning[C]//European Conference on Computer Vision. Springer, Cham, 2016: 785-800. 4. DOTADOTA: A Large-scale Dataset for Object Detection in Aerial ImagesDOTA Dataset Page: https://captain-whu.github.io/DOTA/index.htmlhttp://captain.whu.edu.cn/DOTAweb/https://captain-whu.github.io/ODAI/https://arxiv.org/abs/1711.10398Dota is a large-scale dataset for object detection in aerial images. It can be used to develop and evaluate object detectors in aerial images. For the DOTA-v1.0, as described in the paper, it contains 2806 aerial images from different sensors and platforms. Each image is of the size in the range from about 800 × 800 to 4000 × 4000 pixels and contains objects exhibiting a wide variety of scales, orientations, and shapes. These DOTA images are then annotated by experts in aerial image interpretation using 15 common object categories. The fully annotated DOTA images contains 188, 282 instances, each of which is labeled by an arbitrary (8 d.o.f.) quadrilateral.Reference: Xia G S, Bai X, Ding J, et al. DOTA: A Large-scale Dataset for Object Detection in Aerial Images[J]. arXiv preprint arXiv:1711.10398, 2017. Gui-Song Xia, Xiang Bai, Jian Ding, Zhen Zhu, Serge Belongie, Jiebo Luo, Mihai Datcu, Marcello Pelillo, Liangpei Zhang; The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018, pp. 3974-3983 5. DIUx xView 2018 Detection Challengehttp://xviewdataset.org/http://challenge.xviewdataset.orghttps://arxiv.org/abs/1802.07856xView is one of the largest publicly available sets of overhead imagery. It contains images from complex scenes around the world, annotated with more than one million bounding boxes representing a diverse range of 60 object classes. Compared to other overhead imagery datasets, xView images are high-resolution, multi-spectral, and labeled with a greater variety of objects. Given a high-resolution satellite image, the Challenge task is to predict a bounding box for each object in the image. The DIUx xView Challenge is focused on accelerating progress in four computer vision frontiers: Reduce minimum resolution for detection; Improve learning efficiency; Enable discovery of more object classes; Improve detection of fine-grained classes. The DIUx xView Challenge follows in the footsteps of Challenges such as Common Objects in Context (COCO) and seeks to build off SpaceNet and Functional Map of the World (FMoW) to apply computer vision to the growing amount of available imagery from space so that we can understand the visual world in new ways and address a range of important applications.Reference: Lam D, Kuzma R, McGee K, et al. xView: Objects in Context in Overhead Imagery[J]. arXiv preprint arXiv:1802.07856, 2018. 语义分割1. Zurich Summer Datasethttps://sites.google.com/site/michelevolpiresearch/data/zurich-dataset The “Zurich Summer v1.0” dataset is a collection of 20 chips (crops), taken from a QuickBird acquisition of the city of Zurich (Switzerland) in August 2002. QuickBird images are composed by 4 channels (NIR-R-G-B) and were pansharpened to the PAN resolution of about 0.62 cm GSD. We manually annotated 8 different urban and periurban classes : Roads, Buildings, Trees, Grass, Bare Soil, Water, Railways and Swimming pools. The cumulative number of class samples is highly unbalanced, to reflect real world situations. Note that annotations are not perfect, are not ultradense (not every pixel is annotated) and there might be some errors as well. We performed annotations by jointly selecting superpixels (SLIC) and drawing (freehand) over regions which we could confidently assign an object class.Reference: Volpi, M. &amp; Ferrari, V.; Semantic segmentation of urban scenes by learning local class interactions, In IEEE CVPR 2015 Workshop “Looking from above: when Earth observation meets vision” (EARTHVISION), Boston, USA, 2015. Volpi, M. &amp; Ferrari, V.; Structured prediction for urban scene semantic segmentation with geographic context, In Joint Urban Remote Sensing Event JURSE 2015, Lausanne, Switzerland, 2015. 2. ISPRS Test Project on Urban Classification and 3D Building Reconstruction–2D Semantic Labeling Contesthttp://www2.isprs.org/commissions/comm3/wg4/semantic-labeling.htmlTo this end we provide two state-of-the-art airborne image datasets, consisting of very high resolution true ortho photo (TOP) tiles and corresponding digital surface models (DSMs) derived from dense image matching techniques. Both areas cover urban scenes. While Vaihingen is a relatively small village with many detached buildings and small multi story buildings, Potsdam shows a typical historic city with large building blocks, narrow streets and dense settlement structure. Each dataset has been classified manually into six most common land cover classes. We provide the classification data (label images) for approximately half of the images, while the ground truth of the remaining scenes will remain unreleased and stays with the benchmark test organizers to be used for evaluation of submitted results. Participants shall use all data with ground truth for training or internal evaluation of their method. Six categories/classes have been defined: Impervious surfaces (RGB: 255, 255, 255) Building (RGB: 0, 0, 255) Low vegetation (RGB: 0, 255, 255) Tree (RGB: 0, 255, 0) Car (RGB: 255, 255, 0) Clutter/background (RGB: 255, 0, 0) The clutter/background class includes water bodies (present in two images with part of a river) and other objects that look very different from everything else (e.g., containers, tennis courts, swimming pools) and that are usually not of interest in semantic object classification in urban scenes, however note that participants must submit labels for all classes (including the clutter/background class). For instance, it is not possible to submit only classification results for the category building. 3. 2017 IEEE GRSS Data Fusion Contesthttp://www.grss-ieee.org/2017-ieee-grss-data-fusion-contest/http://dase.ticinumaerospace.com/index.phpThe 2017 Data Fusion Contest will consist in a classification benchmark. The task to perform is classification of land use (more precisely, Local Climate Zones, LCZ, Stewart and Oke, 2012) in various urban environments. Several cities have been selected to test the ability of LCZ prediction at generalizing all over the world. Input data are multi-temporal, multi-source and multi-modal (image and semantic layers). Local climate zones are a generic, climate-based typology of urban and natural landscapes, which delivers information on basic physical properties of an area that can be used by land use planners or climate modelers. LCZ are used as first order discretization of urban areas by the World Urban Database and Access Portal Tools initiative, which aims to collect, store and disseminate data on the form and function of cities around the world. The LCZ classes in this study correspond to those of [Stewart &amp; Oke, 2012]: 10 urban LCZs corresponding to various built types: Compact high-rise (class code in the ground truth: 1); Compact midrise (class code in the ground truth: 2); Compact low-rise (class code in the ground truth: 3); Open high-rise (class code in the ground truth: 4); Open midrise (class code in the ground truth: 5); Open low-rise (class code in the ground truth: 6); Lightweight low-rise (class code in the ground truth: 7); Large low-rise (class code in the ground truth: 8); Sparsely built (class code in the ground truth: 9); Heavy industry (class code in the ground truth: 10). 7 rural LCZs corresponding to various land cover types: Dense trees (class code in the ground truth: 11); Scattered trees (class code in the ground truth: 12); Bush and scrub (class code in the ground truth: 13); Low plants (class code in the ground truth: 14); Bare rock or paved (class code in the ground truth: 15); Bare soil or sand (class code in the ground truth: 16); Water (class code in the ground truth: 17). The contest aims to promote innovation in classification algorithms, as well as to provide objective and fair comparisons among methods. Ranking is based on quantitative accuracy parameters computed with respect to undisclosed test samples from cities unseen during training. Participants will be given a limited time to submit their classification maps after the competition is started. The contest will consist of two steps: Step 1 – training: Participants are provided with five training cities (Berlin, Rome, Paris, Sao Paulo, Hong Kong), including ground truth to train their algorithms. Step 2 – testing on new cities: Participants will receive the data of the test cities and will submit their classification maps by three weeks from the release of this second part of the data set. In parallel, they will submit a short description of the approach used. After evaluation of the results, 4 winners will be announced. The Data: Landsat data, in the form of images with 8 multispectral bands (i.e. visible, short and long infrared wavelengths) resampled at 100m resolution (courtesy of the U.S. Geological Survey); Sentinel2 images, with 9 multispectral bands (i.e. visible, vegetation red edges and short infrared wavelengths) resampled at 100m resolution (Contains modified Copernicus Data 2016); participants are encouraged to use the full resolution data, for which a direct link is provided in the data package. Ancillary data: Open Street Map (OSM) layers with land use information: building, natural, roads and land-use areas. We also provide rasterized versions of OSM layers at 20m resolution for building and land-use areas, superimposable with the satellite images. Moreover, for the training cities only, we also provide ground-truth of the various LCZ classes on several areas of the city (defined as polygons using the class codes above). They are provided as raster layers at 100m resolution, superimposable to the satellite images. The ground-truth for the test set will remain undisclosed and will be used for evaluation of the results. 4. 2018 IEEE GRSS Data Fusion Contest–Advanced multi-sensor optical remote sensing for urban land use and land cover classificationhttp://www.grss-ieee.org/community/technical-committees/data-fusion/2018-ieee-grss-data-fusion-contest/http://dase.ticinumaerospace.comThe 2018 IEEE GRSS Data Fusion Contest, organized by the Image Analysis and Data Fusion Technical Committee, aims to promote progress on fusion and analysis methodologies for multi-source remote sensing data. The 2018 Data Fusion Contest consists of a classification benchmark. The task to be performed is urban land use and land cover classification. To test their synergy as well as individual potential for urban land use and land cover classification, classification results can be submitted to three parallel and independent competitions: Data Fusion Classification Challenge (use of at least two data sets) Multispectral LiDAR Classification Challenge Hyperspectral Classification Challenge The Data:We provide (Acquired by the National Center for Airborne Laser Mapping, NCALM): The data were acquired by NCALM on February 16, 2017 between 16:31 and 18:18 GMT. Sensors used in this campaign include an Optech Titam MW (14SEN/CON340) with integrated camera (a LIDAR sensor operating at three different laser wavelengths), a DiMAC ULTRALIGHT+ (a very high resolution color imager) with a 70 mm focal length, and an ITRES CASI 1500 (a hyperspectral imager). Multispectral-LiDAR point cloud data at 1550 nm, 1064 nm, and 532 nm; Intensity rasters from first return per channel and DSMs at a 50-cm GSD. Hyperspectral data covering a 380-1050 nm spectral range with 48 bands at a 1-m GSD. Very high resolution RGB imagery at a 5-cm GSD. The image is organized into several separate tiles. The data were acquired by NCALM on February 16, 2017 between 16:31 and 18:18 GMT. Sensors used in this campaign include an Optech Titam MW (14SEN/CON340) with integrated camera (a LIDAR sensor operating at three different laser wavelengths), a DiMAC ULTRALIGHT+ (a very high-resolution color imager) with a 70mm focal length, and an ITRES CASI 1500 (a hyperspectral imager). The sensors were aboard a Piper PA-31- 350 Navajo Chieftain aircraft. Moreover, for the training region only, we also provide ground-truth corresponding to 20 urban land use and land cover classes. They are provided as raster at a 0.5-m GSD, superimposable to airborne images. The ground truth for the test set remains undisclosed and will be used for evaluation of the results. Urban Land Use and Land Cover Classes: 0 – Unclassified 1 – Healthy grass 2 – Stressed grass 3 – Artificial turf 4 – Evergreen trees 5 – Deciduous trees 6 – Bare earth 7 – Water 8 – Residential buildings 9 – Non-residential buildings 10 – Roads 11 – Sidewalks 12 – Crosswalks 13 – Major thoroughfares 14 – Highways 15 – Railways 16 – Paved parking lots 17 – Unpaved parking lots 18 – Cars 19 – Trains 20 – Stadium seats 5. EvLab-SS Datasethttp://earthvisionlab.whu.edu.cn/zm/SemanticSegmentation/index.htmlThe EvLab-SS benchmark is designed for the evaluation of the semantic segmentation algorithms on real engineered scenes, which aims to find a good deep learning architecture for the high resolution pixel-wise classification task in remote sensing area.The dataset is originally obtained from the Chinese Geographic Condition Survey and Mapping Project, and each image is fully annotated by the Geographic Conditions Survey (NO.GDPJ 01—2013) standards. The average resolution of the dataset is approximately 4500 × 4500 pixels. The EvLab-SS dataset contains 11 major classes, namely, background, farmland, garden, woodland, grassland, building, road, structures, digging pile, desert and waters, and currently includes 60 frames of images captured by different platforms and sensors. The dataset comprises 35 satellite images, 19 frames of which are captured by the World-View-2 satellite (re-sample GSD 0.2 m), 5 frames are captured by the GeoEye satellite (re-sample GSD 0.5 m), 5 frames are captured by the QuickBird satellite (re-sample GSD 2 m), 6 frames are captured by the GF-2 satellite (re-sample GSD 1 m). The dataset also has 25 aerial images, 10 images of which with spatial resolution of 0.25 m and 15 images have a spatial resolution of 0.1 m.Reference: Zhang M, Hu X, Zhao L, et al. Learning dual multi-scale manifold ranking for semantic segmentation of high-resolution images[J]. Remote Sensing, 2017, 9(5): 500. 6. DeepGlobe Land Cover Classification Challengehttp://deepglobe.org/index.htmlhttps://competitions.codalab.org/competitions/18468Automatic categorization and segmentation of land cover is of great importance for sustainable development, autonomous agriculture, and urban planning. We would like to introduce the challenge of automatic classification of land cover types. This problem is defined as a multi-class segmentation task to detect areas of urban, agriculture, rangeland, forest, water, barren, and unknown. The evaluation will be based on the accuracy of the class labels.Reference: Demir I, Koperski K, Lindenbaum D, et al. DeepGlobe 2018: A Challenge to Parse the Earth through Satellite Images[J]. arXiv preprint arXiv:1805.06561, 2018. 建筑物检测1. Massachusetts Buildings Datasethttps://www.cs.toronto.edu/~vmnih/data/ The size of all images in these datasets is 1500×1500, and the resolution is 1m2/pixel. The buildingdataset consists of 137 sets of aerial images and corresponding single-channel label images for training part, 10 for testing part, and 4 for validation part. Reference: Mnih V. Machine learning for aerial image labeling[D]. University of Toronto (Canada), 2013. 2. SpaceNet Buildings Datasethttps://spacenetchallenge.github.io/The Data - Over 685,000 footprints across the Five SpaceNet Areas of Interest. AOI Area of Raster (Sq. Km) Building Labels (Polygons) AOI_1_Rio 2,544 382,534 AOI_2_Vegas 216 151,367 AOI_3_Paris 1,030 23,816 AOI_4_Shanghai 1,000 92,015 AOI_5_Khartoum 765 35,503 SpaceNet Buildings Dataset Round 1CatalogThe data is hosted on AWS in a requester pays bucket.1aws s3 ls s3://spacenet-dataset/SpaceNet_Buildings_Dataset_Round1/ Training DataAOI 1 - Rio - Building Extraction TrainingTo download processed 200mx200m tiles of AOI 1 (23 GB) with associated building footprints for training do the following:12aws cp s3://spacenet-dataset/spacenet_TrainData/3band.tar.gz .aws cp s3://spacenet-dataset/spacenet_TrainData/8band.tar.gz . Test DataAOI 1 - Rio - Building Extraction TestingTo download processed 200mx200m tiles of AOI 1 (7.9 GB) for testing do:12aws cp s3://spacenet-dataset/spacenet_TestData/3band.tar.gz .aws cp s3://spacenet-dataset/spacenet_TestData/8band.tar.gz . SpaceNet Buildings Dataset Round 2CatalogThe data is hosted on AWS in a requester pays bucket.1aws s3 ls s3://spacenet-dataset/SpaceNet_Buildings_Dataset_Round2/ Training Data (57.3 GB)1aws s3 cp s3://spacenet-dataset/SpaceNet_Buildings_Dataset_Round2/spacenetV2_Train/ . --recursive AOI 2 - Vegas - Building Extraction TrainingTo download processed 200mx200m tiles of AOI 2 (23 GB) with associated building footprints for training do the following:1aws s3 cp s3://spacenet-dataset/SpaceNet_Buildings_Dataset_Round2/spacenetV2_Train/AOI_2_Vegas_Train.tar.gz . AOI 3 - Paris - Building Extraction TrainingTo download processed 200mx200m tiles of AOI 3 (5.3 GB) with associated building footprints do the following:1aws s3 cp s3://spacenet-dataset/SpaceNet_Buildings_Dataset_Round2/spacenetV2_Train/AOI_3_Paris_Train.tar.gz . AOI 4 - Shanghai - Building Extraction TrainingTo download processed 200mx200m tiles of AOI 4 (23.4 GB) with associated building footprints do the following:1aws s3 cp s3://spacenet-dataset/SpaceNet_Buildings_Dataset_Round2/spacenetV2_Train/AOI_4_Shanghai_Train.tar.gz . AOI 5 - Khartoum - Building Extraction TrainingTo download processed 200mx200m tiles of AOI 2 (4.7 GB) with associated building footprints do the following:1aws s3 cp s3://spacenet-dataset/SpaceNet_Buildings_Dataset_Round2/spacenetV2_Train/AOI_5_Khartoum_Train.tar.gz . Test Data (19 GB)1aws s3 cp s3://spacenet-dataset/SpaceNet_Buildings_Dataset_Round2/spacenetV2_Test/ . --recursive AOI 2 - Vegas - Building Extraction TestingTo download processed 200mx200m tiles of AOI 2 (7.9 GB) for testing do:1aws s3 cp s3://spacenet-dataset/SpaceNet_Buildings_Dataset_Round2/spacenetV2_Test/AOI_2_Vegas_Test_public.tar.gz . AOI 3 - Paris - Building Extraction TestingTo download processed 200mx200m tiles of AOI 3 (1.8 GB) for testing do:1aws s3 cp s3://spacenet-dataset/SpaceNet_Buildings_Dataset_Round2/spacenetV2_Test/AOI_3_Paris_Test_public.tar.gz . AOI 4 - Shanghai - Building Extraction TestingTo download processed 200mx200m tiles of AOI 4 (7.7 GB) for testing do:1aws s3 cp s3://spacenet-dataset/SpaceNet_Buildings_Dataset_Round2/spacenetV2_Test/AOI_4_Shanghai_Test_public.tar.gz . AOI 5 - Khartoum - Building Extraction TestingTo download processed 200mx200m tiles of AOI 2 (1.6 GB) for testing do:1aws s3 cp s3://spacenet-dataset/SpaceNet_Buildings_Dataset_Round2/spacenetV2_Test/AOI_5_Khartoum_Test_public.tar.gz . 3. Inria Aerial Image Labeling Datasethttps://project.inria.fr/aerialimagelabeling/The Inria Aerial Image Labeling addresses a core topic in remote sensing: the automatic pixelwise labeling of aerial imagery (link to paper). Dataset features: Coverage of 810 km² (405 km² for training and 405 km² for testing)Aerial orthorectified color imagery with a spatial resolution of 0.3 mGround truth data for two semantic classes: building and not building (publicly disclosed only for the training subset)The images cover dissimilar urban settlements, ranging from densely populated areas (e.g., San Francisco’s financial district) to alpine towns (e.g,. Lienz in Austrian Tyrol). Instead of splitting adjacent portions of the same images into the training and test subsets, different cities are included in each of the subsets. For example, images over Chicago are included in the training set (and not on the test set) and images over San Francisco are included on the test set (and not on the training set). The ultimate goal of this dataset is to assess the generalization power of the techniques: while Chicago imagery may be used for training, the system should label aerial images over other regions, with varying illumination conditions, urban landscape and time of the year.Reference: Maggiori E, Tarabalka Y, Charpiat G, et al. Can semantic labeling methods generalize to any city? The INRIA aerial image labeling benchmark[C]//IEEE International Symposium on Geoscience and Remote Sensing (IGARSS). 2017. 4. The USSOCOM Urban 3D Challengehttps://www.topcoder.com/urban3dhttps://spacenetchallenge.github.io/datasets/Urban_3D_Challenge_summary.htmlhttps://github.com/topcoderinc/Urban3dThis challenge published a large-scale dataset containing 2D orthrorectified RGB and 3D Digital Surface Models and Digital Terrain Models generated from commercial satellite imagery covering over 360 km of terrain and containing roughly 157,000 annotated building footprints. All imagery products are provided at 50 cm ground sample distance (GSD). This unique 2D/3D large scale dataset provides researchers an opportunity to utilize machine learning techniques to further improve state of the art performance. Reliable labeling of buildings based on satellite imagery is one of the first and most challenging steps in producing accurate 3D models and maps. While automated algorithms continue to improve, significant manual effort is still necessary to ensure geospatial accuracy and acceptable quality. Improved automation is required to enable more rapid response to major world events such as humanitarian and disaster response. 3D height data can help improve automated building labeling performance, and capabilities for providing this data on a global scale are now emerging. In this challenge, we ask solvers to use satellite imagery and newly available 3D height data products to improve upon the state of the art for automated building detection and labeling. USSOCOM is seeking an algorithm that provides reliable, automatic labeling of buildings based solely on orthorectified color satellite imagery and 3D height data.Reference: H. Goldberg, M. Brown, and S. Wang, A Benchmark for Building Footprint Classification Using Orthorectified RGB Imagery and Digital Surface Models from Commercial Satellites, 46th Annual IEEE Applied Imagery Pattern Recognition Workshop, Washington, D.C, 2017. H. Goldberg, S. Wang, M. Brown, and G. Christie. Urban 3D Challenge: Building Footprint Detection Using Orthorectified Imagery and Digital Surface Models from Commercial Satellites. In Proceedings SPIE Defense and Commercial Sensing: Geospatial Informatics and Motion Imagery Analytics VIII, Orlando, Florida, USA, 2018. 5. DeepGlobe Building Extraction Challengehttp://deepglobe.org/index.htmlhttps://competitions.codalab.org/competitions/18544Modeling population dynamics is of great importance for disaster response and recovery, and detection of buildings and urban areas are key to achieve so. We would like to pose the challenge of automatically detecting buildings from satellite images. This problem is formulated as a binary segmentation problem to localize all building polygons in each area. The evaluation will be based on the overlap of detected polygons with the ground truth.Reference: Demir I, Koperski K, Lindenbaum D, et al. DeepGlobe 2018: A Challenge to Parse the Earth through Satellite Images[J]. arXiv preprint arXiv:1805.06561, 2018. 6. CrowdAI Mapping Challengehttps://www.crowdai.org/challenges/mapping-challengeIn this challenge you will be provided with a dataset of individual tiles of satellite imagery as RGB images, and their corresponding annotations of where an image is there a building. The goal is to train a model which given a new tile can annotate all buildings.Datasets train.tar.gz : This is the Training Set of 280741 tiles (as 300x300 pixel RGB images) of satellite imagery, along with their corresponding annotations in MS-COCO format val.tar.gz: This is the suggested Validation Set of 60317 tiles (as 300x300 pixel RGB images) of satellite imagery, along with their corresponding annotations in MS-COCO format test_images.tar.gz : This is the Test Set for Round-1, where you are provided with 60697 files (as 300x300 pixel RGB images) and your are required to submit annotations for all these files. 7. WHU Building Datasethttp://study.rsgis.whu.edu.cn/pages/download/This dataset consists of an aerial dataset and a satellite dataset. 1、 Aerial imagery datasetThe original aerial data comes from the New Zealand Land Information Services website. We manually edited Christchurch’s building vector data, with about 22,000 independent buildings. The original ground resolution of the images is 0.075m. Due to the size of the aerial imagery (about 23G), we provide manually edited shapefile. The aerial image can be downloaded from the official website: https://data.linz.govt.nz/layer/51932-christchurch-post-earthquake-01m-urban-aerial-photos-24-february-2011/. For those who are not familiar with GIS and shapefile, we down-sampled the most parts of aerial images (including 18,7000 buildings) to 0.3m ground resolution, and cropped them into 8,188 tiles with 512×512 pixels. The shapefile is additionally rasterized. The ready-to-use samples are divided into two parts: a training set (14,5000 buildings) and a test set (42,000 buildings). 2、 satellite imagery datasetThe satellite imagery dataset is collected globally from various remote sensing resources including Quickbird, Worldview series, IKONOS, ZY-3, etc. We manually delineated all the buildings. The satellite imagery dataset contains 204 satellite images and the corresponding ground truth images (512 × 512 tiles with resolutions varying from 0.3 m to 2.5 m). it is ready-to-use as cropped aerial dataset. 道路检测1. Massachusetts Roads Datasethttps://www.cs.toronto.edu/~vmnih/data/The size of all images in these datasets is 1500×1500, and the resolution is 1m2/pixel. The road dataset consists of 1108 sets for training part, 49 for testingpart, and 14 for validation part.Reference: Mnih V. Machine learning for aerial image labeling[D]. University of Toronto (Canada), 2013. 2. SpaceNet Roads Datasethttps://spacenetchallenge.github.io/The Data - Over 8000 Km of roads across the four SpaceNet Areas of Interest. AOI Area of Raster (Sq. Km) Road Centerlines (LineString) AOI_2_Vegas 216 3685 km AOI_3_Paris 1,030 425 km AOI_4_Shanghai 1,000 3537 km AOI_5_Khartoum 765 1030 km Road Type Breakdown (km of Road) Road Type AOI_2_Vegas AOI_3_Paris AOI_4_Shanghai AOI_5_Khartoum Total Motorway 115 9 102 13 240 Primary 365 14 192 98 669 Secondary 417 58 501 66 1042 Tertiary 3 11 34 68 115 Residential 1646 232 939 485 3301 Unclassified 1138 95 1751 165 3149 Cart track 2 6 19 135 162 Total 3685 425 3537.9 1030 8677 CatalogThe data is hosted on AWS in a requester pays bucket.1aws s3 ls s3://spacenet-dataset/SpaceNet_Roads_Competition/ --request-payer requester Sample Data10 Samples from each AOI - Road Network Extraction SamplesTo download processed 400mx400m tiles of AOI 2 (728.8 MB) with associated road centerlines for training do the following:1aws s3api get-object --bucket spacenet-dataset --key SpaceNet_Roads_Competition/SpaceNet_Roads_Sample.tar.gz --request-payer requester SpaceNet_Roads_Sample.tar.gz Training DataAOI 2 - Vegas - Road Network Extraction TrainingTo download processed 400mx400m tiles of AOI 2 (25 GB) with associated road centerlines for training do the following:1aws s3api get-object --bucket spacenet-dataset --key SpaceNet_Roads_Competition/AOI_2_Vegas_Roads_Train.tar.gz --request-payer requester AOI_2_Vegas_Roads_Train.tar.gz AOI 3 - Paris - Road Network Extraction TrainingTo download processed 400mx400m tiles of AOI 3 (5.6 GB) with associated road centerlines for training do the following:1aws s3api get-object --bucket spacenet-dataset --key SpaceNet_Roads_Competition/AOI_3_Paris_Roads_Train.tar.gz --request-payer requester AOI_3_Paris_Roads_Train.tar.gz AOI 4 - Shanghai - Road Network Extraction TrainingTo download processed 400mx400m tiles of AOI 4 (25 GB) with associated road centerlines for training do the following:1aws s3api get-object --bucket spacenet-dataset --key SpaceNet_Roads_Competition/AOI_4_Shanghai_Roads_Train.tar.gz --request-payer requester AOI_4_Shanghai_Roads_Train.tar.gz AOI 5 - Khartoum - Road Network Extraction TrainingTo download processed 400mx400m tiles of AOI 5 (25 GB) with associated road centerlines for training do the following:1aws s3api get-object --bucket spacenet-dataset --key SpaceNet_Roads_Competition/AOI_5_Khartoum_Roads_Train.tar.gz --request-payer requester AOI_5_Khartoum_Roads_Train.tar.gz Test DataAOI 2 - Vegas - Road Network Extraction TestingTo download processed 400mx400m tiles of AOI 2 (8.1 GB) for testing do:1aws s3api get-object --bucket spacenet-dataset --key SpaceNet_Roads_Competition/AOI_2_Vegas_Roads_Test_Public.tar.gz --request-payer requester AOI_2_Vegas_Roads_Test_Public.tar.gz AOI 3 - Paris - Road Network Extraction TestingTo download processed 400mx400m tiles of AOI 3 (1.9 GB) for testing do:1aws s3api get-object --bucket spacenet-dataset --key SpaceNet_Roads_Competition/AOI_3_Paris_Roads_Test_Public.tar.gz --request-payer requester AOI_3_Paris_Roads_Test_Public.tar.gz AOI 4 - Shanghai - Road Network Extraction TestingTo download processed 400mx400m tiles of AOI 4 (8.1 GB) for testing do:1aws s3api get-object --bucket spacenet-dataset --key SpaceNet_Roads_Competition/AOI_4_Shanghai_Roads_Test_Public.tar.gz --request-payer requester AOI_4_Shanghai_Roads_Test_Public.tar.gz AOI 5 - Khartoum - Road Network Extraction TestingTo download processed 400mx400m tiles of AOI 5 (8.1 GB) for testing do:1aws s3api get-object --bucket spacenet-dataset --key SpaceNet_Roads_Competition/AOI_5_Khartoum_Roads_Test_Public.tar.gz --request-payer requester AOI_5_Khartoum_Roads_Test_Public.tar.gz 3. DeepGlobe Road Extraction Challengehttp://deepglobe.org/index.htmlhttps://competitions.codalab.org/competitions/18467In disaster zones, especially in developing countries, maps and accessibility information are crucial for crisis response. We would like to pose the challenge of automatically extracting roads and street networks from satellite images.Reference: Demir I, Koperski K, Lindenbaum D, et al. DeepGlobe 2018: A Challenge to Parse the Earth through Satellite Images[J]. arXiv preprint arXiv:1805.06561, 2018. 变化检测1. Onera Satellite Change Detectionhttp://dase.ticinumaerospace.comThe Onera Satellite Change Detection (OSCD) dataset address the issue of detecting changes between satellite images at different dates.It comprises 24 pairs of multispectral images taken from the Sentinel-2 satellite in 2015 and 2018. Locations are picked all over the world, in North and South America, Europe, Middle-East and Asia. For each location, registered pairs of 13-band multispectral satellite images obtained by the Sentinel-2 satellites are provided. Images vary in spatial resolution between 10m, 20m and 60m. Pixel-level change groundtruth is provided for 14 of the image pairs. The annotated changes focus on urban changes, such as new buildings or new roads. These data can be used for training and setting parameters of change detection algorithms.References:Urban Change Detection for Multispectral Earth Observation Using Convolutional Neural Networks R. Caye Daudt, B. Le Saux, A. Boulch, Y. Gousseau IEEE International Geoscience and Remote Sensing Symposium (IGARSS’2018) Valencia, Spain, July 2018 其他数据集RSICDhttps://github.com/201528014227051/RSICD_optimalRSICD is used for remote sensing image captioning task. more than ten thousands remote sensing images are collected from Google Earth, Baidu Map, MapABC, Tianditu. The images are fixed to 224X224 pixels with various resolutions. The total number of remote sensing images are 10921, with five sentences descriptions per image. To the best of our knowledge, this dataset is the largest dataset for remote sensing captioning. The sample images in the dataset are with high intra-class diversity and low inter-class dissimilarity. Thus, this dataset provides the researchers a data resource to advance the task of remote sensing captioning.Reference: Lu X, Wang B, Zheng X, et al. Exploring Models and Data for Remote Sensing Image Caption Generation[J]. IEEE Transactions on Geoscience and Remote Sensing, 2017. The IARPA Functional Map of the World (fMoW) Challengehttps://www.iarpa.gov/challenges/fmow.htmlhttps://spacenetchallenge.github.io/datasets/fmow_summary.htmlhttps://github.com/fmowIntelligence analysts, policy makers, and first responders around the world rely on geospatial land use data to inform crucial decisions about global defense and humanitarian activities. Historically, analysts have manually identified and classified geospatial information by comparing and analyzing satellite images, but that process is time consuming and insufficient to support disaster response. The fMoW Challenge sought to foster breakthroughs in the automated analysis of overhead imagery by harnessing the collective power of the global data science and machine learning communities; empowering stakeholders to bolster their capabilities through computer vision automation. The challenge published one of the largest publicly available satellite-image datasets to date, with more than one million points of interest from around the world. The dataset also contains other elements such as temporal views, multispectral imagery, and satellite-specific metadata that researchers can exploit to build novel algorithms capable of classifying facility, building, and land use.Reference: Gordon Christie, Neil Fendley, James Wilson, Ryan Mukherjee; The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018, pp. 6172-6180 IARPA Multi-View Stereo 3D Mapping Challengehttps://www.iarpa.gov/challenges/3dchallenge.htmlhttps://spacenetchallenge.github.io/datasets/mvs_summary.htmlThis data set includes DigitalGlobe WorldView-3 panchromatic and multispectral images of a 100 square kilometer area near San Fernando, Argentina. We also provide 20cm airborne lidar ground truth data for a 20 square kilometer subset of this area and performance analysis software to assess accuracy and completeness metrics. Commercial satellite imagery is provided courtesy of DigitalGlobe, and ground truth lidar is provided courtesy of IARPA.Catalog1aws s3 ls s3://spacenet-dataset/mvs_dataset The catalog contains the following packages: Updated metric analysis software with examples from contest winners Challenge data package with instructions, cropped TIFF images, ground truth, image cropping software, and metric scoring software (1.2 GB) JHU/APL example MVS solution (451 MB) NITF panchromatic, multispectral, and short-wave infrared DigitalGlobe WorldView-3 satellite images (72.1 GB) LAZ lidar point clouds with SBET (2.2 GB) Spectral image calibration software (84 MB)]]></content>
  </entry>
  <entry>
    <title><![CDATA[Context Encoding for Semantic Segmentation(CVPR2018)]]></title>
    <url>%2F2018%2F06%2F11%2FContext-Encoding-for-Semantic-Segmentation%2F</url>
    <content type="text"><![CDATA[提出了Context Encoding Module来捕获场景的语义上下文并选择性地强调与类别相关的特征图，所提出的EncNet实现了新的state-of-the-art的结果。在PASCAL-Context达到51.7% mIoU, 在PASCAL VOC 2012达到85.9% mIoU，单个模型在ADE20K test set达到0.5567,超过了COCO-Place Challenge 2017的冠军。Context Encoding Module也可以改善用于分类的相对较浅的网络的特征表达，在CIFAR-10上14层的网络实现了3.45%的错误率与state-of-the-art approaches的10倍多层网络的精度相当。 paper: https://arxiv.org/abs/1803.08904http://openaccess.thecvf.com/content_cvpr_2018/html/Zhang_Context_Encoding_for_CVPR_2018_paper.htmlcode: https://github.com/zhanghang1989/PyTorch-Encodingauthor：Hang Zhang, Kristin Dana, Jianping Shi, Zhongyue Zhang, Xiaogang Wang, Ambrish Tyagi, Amit Agrawal 1. Introduction最近语义分割网络使用基于多分辨率金字塔来扩大感受野，如PSPSNet中的Spatial Pyramid Pooling和Deeplab中的Atrous Spatial Pyramid Pooling。虽然这些方法可以提升性能，但是上下文地表达不明确，导致了一个问题：增加感受野就等同于整合了上下文信息了吗？如图所示是来自于ADE20K中的一张图片，包含150个类别。如果允许标注者先选择图像的语境(如卧室)，接着工具会提供一个小的相关的类别列表(如床，椅子等)，这会极大地减少可能的类别搜索空间。同样，如果可以设计一个方法来充分利用场景语境类别的概率的关系，对网络来说语义分割会更容易。是否可以把传统方法的上下文编码和深度学习结合？之前作者就设计了一个Encoding Layer整合字典学习和残差编码过程到一个CNN层中来捕获无序的表达，在纹理分类中获得了state-of-the-art的结果。在这篇论文中作者延申Encoding Layer来捕获全局特征统计信息来理解语义上下文。这篇论文的两个贡献： 提出了Context Encoding Module。在Context Encoding Module中还包含Semantic Encoding Loss(SE-Loss)，一个利用全局场景上下文信息的单元。Context Encoding Module可以捕获场景的语义上下文并选择性地强调与类别相关的特征图。常规的训练过程中只用了逐像素的分割loss，没有利用场景的上下文。引入Semantic Encoding Loss(SE-Loss)来正则化训练，让网络预测在场景中物体种类的出现概率来强制网络学习语义上下文。而且SE-Loss同等对待大物体和小物体，发现小物体的实际分割效果有提升。 设计了一个语义分割网络Context Encoding Network (EncNet)。EncNet在预训练的ResNet中通过引入了Context Encoding Module强化网络。使用了dilation convolution。2. Context Encoding ModuleEncoding Layer输入为$H×W×C$的特征图，之后reshape为$N(H×W)$个$C$维的vector $X=\lbrace{x_1,…,x_N}\rbrace$，在Encoding Layer中要学习codebook $D=\lbrace{d_1,…,d_K}\rbrace$其中包含K个codeword以及smoothing factor $S=\lbrace{s_1,…,s_K}\rbrace$。Encoding Layer输出为残差编码，残差编码是由soft-assignment weights的残差得到的。残差计算方法：$r_{ik}=x_i-d_k$soft-assignment weights计算方法：]]></content>
      <categories>
        <category>Semantic Segmentation</category>
      </categories>
      <tags>
        <tag>Semantic Segmentation</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ESPNet: Efficient Spatial Pyramid of Dilated Convolutions for Semantic Segmentation]]></title>
    <url>%2F2018%2F06%2F10%2FESPNet-Efficient-Spatial-Pyramid-of-Dilated-Convolutions-for-Semantic-Segmentation%2F</url>
    <content type="text"><![CDATA[作者提出了efﬁcient spatial pyramid(ESP) module构建了一个轻量级的语义分割网络。 project page: https://sacmehta.github.io/ESPNet/paper: https://arxiv.org/abs/1803.06815code: https://github.com/sacmehta/ESPNet ESP module在ESP module中将一个标准卷积分解为2步：point-wise convolution 和 spatial pyramid of dilated convolution。point-wise convolution：特征图通道维度由M变为$d=\frac{N}{K}$spatial pyramid of dilated convolution：分为K个分支，每个分支的dilation为$2^{K-1}$的卷积,最后concatenate在一起.ESP module的参数量为$\frac{MN}{K}+\frac{(nN)^2}{K}$，感受野为$[(n-1)2^{K-1}+1]^2$。而标准卷积的参数量为$n^2NM$ Hierarchical feature fusion (HFF)为了解决由于引入dilated convolution带来的网格效应，将不同dilation的特征图分层求和，然后再concatenate。 与其他网络建立策略的比较 ESP module： reduce-split-transform-merge MobileNet module：depth-wise convolutions (transform) and point-wise convolutions (expand)。当K=N时，除了卷积操作的顺序不同外，和MobileNet module一样。 ShufﬂeNet module：reduce-transform-expand。将原来ResNet中的bottleneck block中1×1的卷积和3×3的卷积替换为1×1的grouped convolution和3×3的depth-wise convolution。 Inception module：f split-reduce-transform-merge ResNext module：split-reduce-transform-expand-merge Atrous spatial pyramid (ASP) module：split-transform-merge Network structure]]></content>
      <categories>
        <category>Semantic Segmentation</category>
      </categories>
      <tags>
        <tag>Semantic Segmentation</tag>
        <tag>Real-Time</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Dynamic-structured Semantic Propagation Network(CVPR2018)]]></title>
    <url>%2F2018%2F06%2F10%2FDynamic-structured-Semantic-Propagation-Network%2F</url>
    <content type="text"><![CDATA[Paper: https://arxiv.org/abs/1803.06067http://openaccess.thecvf.com/content_cvpr_2018/html/Liang_Dynamic-Structured_Semantic_Propagation_CVPR_2018_paper.html]]></content>
      <categories>
        <category>Semantic Segmentation</category>
      </categories>
      <tags>
        <tag>Semantic Segmentation</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[RTSeg: Real-time Semantic Segmentation Comparative Study(Accepted in IEEE ICIP 2018)ShuffleSeg：Real-time Semantic Segmentation Network]]></title>
    <url>%2F2018%2F06%2F10%2FRTSeg-Real-time-Semantic-Segmentation-Comparative-Study%2F</url>
    <content type="text"><![CDATA[在第一篇论文中，作者将语义分割网络分为特征提取阶段和解码阶段，比较了了不同种特征提取模块和解码模块的组合来达到实时语义分割。 特征提取模块：VGG16，ResNet18，MobileNet，ShuffleNet解码模块：SkipNet，UNet，Dilation Frontend 在第二篇论文中，作者受ShuffleNet启发，提出了ShuffleSeg语义分割网络，在encoding阶段利用了group convolution和channel shufﬂing，比较了不同的decoding方法，发现UNet可以达到最好的精度，skip architecture可以在精度和实时性取得较好的权衡。 Paper: https://arxiv.org/abs/1803.02758 https://arxiv.org/abs/1803.03816Code: https://github.com/MSiam/TFSegmentation RTSeg: Real-time Semantic Segmentation Comparative Study ShuffleSeg：Real-time Semantic Segmentation Network]]></content>
      <categories>
        <category>Semantic Segmentation</category>
      </categories>
      <tags>
        <tag>Semantic Segmentation</tag>
        <tag>Real-Time</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[DeepLab V3+——Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation]]></title>
    <url>%2F2018%2F06%2F03%2FEncoder-Decoder-with-Atrous-Separable-Convolution-for-Semantic-Image-Segmentation%2F</url>
    <content type="text"><![CDATA[在DeepLab V3+中通过采用了encoder-decoder结构，在DeepLab V3中加入了一个简单有效的decoder模块来改善物体边缘的分割结果。除此之外还尝试使用Xception作为encoder，在Atrous Spatial Pyramid Pooling和decoder中应用depth-wise separable convolution得到了更快精度更高的网络，在PASCAL VOC 2012数据集上达到state-of-art的效果。 Paper: https://arxiv.org/abs/1802.02611 Code: https://github.com/tensorflow/models/tree/master/research/deeplab 1. Introduction在Introduction和related works中，作者回顾语义分割中的常用的两种结构： 空间金字塔池化 encoder-decoder空间金字塔池化(图a)可以池化不同分辨率的特征图来捕获丰富的上下文信息，而encoder-decoder结构(图b)则可以获得锋利的边界。因此，在DeepLab V3+中通过采用了encoder-decoder结构，在DeepLab V3中加入了一个简单有效的decoder模块来改善物体边缘的分割结果(图c)：先上采样4倍，在与encoder中的特征图concatenate，最后在上采样4倍恢复到原始图像大小。除此之外还尝试使用Xception作为encoder，在Atrous Spatial Pyramid Pooling和decoder中应用depth-wise separable convolution得到了更快精度更高的网络。 2. Methods encoderencoder就是DeepLab V3，通过修改ResNet101最后两(一)个block的stride，使得output stride为8(16)。之后在block4后应用改进后的Atrous Spatial Pyramid Pooling，将所得的特征图concatenate用1×1的卷积得到256个通道的特征图。 decoder在decoder中，特征图首先上采样4倍，然后与encoder中对应分辨率低级特征concatenate。在concatenate之前，由于低级特征图的通道数通常太多(256或512)，而从encoder中得到的富含语义信息的特征图通道数只有256，这样会淡化语义信息，因此在concatenate之前，需要将低级特征图通过1×1的卷积减少通道数。在concatenate之后用3×3的卷积改善特征，最后上采样4倍恢复到原始图像大小。 将Xception作为encoder原始的Xception结构如下：采用的Xception模型为MSRA team提出的改进的Xception，叫做Aligned Xception，并做了几点修改： 网络深度与Aligned Xception相同，不同的地方在于不修改entry flow network的结构，为了快速计算和有效的使用内存。 所有的max pooling操作替换成带stride的separable convolution，这能使得对任意分辨率的图像应用atrous separable convolution提取特征。 在每个3×3的depath-wise convolution后增加BN层和ReLU。 3. Experimental Evaluationlearning rate schedule：polyinitial learning rate 0.007crop size 513×513 3.1. Decoder Design Choicesdecoder的设计主要考虑三点： 1×1卷积的通道数。最后采用了48。 用来获得更锋利的边界的3×3的卷积。最后采用了2个3×3的卷积。在卷积类型方面，实验发现2个3×3的卷积效果要比1个或者3个3×3的卷积，或者卷积核为1×1的效果好。 所使用的encoder的低级特征。当同时使用conv2和conv3的特征时，先上采样2倍与conv3的特征concatenate，然后再上采样2倍与conv2的特征concatenate。这种结构并没有观察到显著的改进。因此最后只利用了conv2的特征。 3.2. ResNet-101 as Network Backbone 3.3. Xception as Network Backbone首先在ImageNet上预训练Xception。 官方执行结果link：https://github.com/tensorflow/models/blob/master/research/deeplab/g3doc/model_zoo.md 3.4. Improvement along Object Boundaries使用trimap实验测量模型在分割边界的准确度。计算边界周围扩展频带(称为trimap)内的mIoU。结果如下：附MobileNet V2应用DeepLab V3+的结果]]></content>
      <categories>
        <category>Semantic Segmentation</category>
      </categories>
      <tags>
        <tag>Semantic Segmentation</tag>
        <tag>Dilation Convolution</tag>
        <tag>encoder-decoder</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Stacked Deconvolutional Network for Semantic Segmentation]]></title>
    <url>%2F2018%2F06%2F01%2FStacked-Deconvolutional-Network-for-Semantic-Segmentation%2F</url>
    <content type="text"><![CDATA[作者是中科院自动化所CASIA_IVA_JD团队，在COCO Places Challenges 2017 Scene Parsing取得了冠军，其中就运用了这个网络。提出了Stacked Deconvolutional Network(SDN)，网络的第一个encoder为DenseNet161，后续SDN通过堆叠多个浅层的Deconvolutional Network来整合上下文信息和恢复位置信息，为了帮助网络训练和特征重用加入了inter-unit连接和intra-unit连接，同时用hierarchical supervision使网络各个部分学习到有判别力发的特征表达，有利于网络优化。 Paper: https://arxiv.org/abs/1708.04943Slide：http://presentations.cocodataset.org/Places17-CASIA_IVA_JD.pdf 比赛中采用了SDN，改进后的DeepLabV3和ResNet38三个模型集成。]]></content>
      <categories>
        <category>Semantic Segmentation</category>
      </categories>
      <tags>
        <tag>Semantic Segmentation</tag>
        <tag>encoder-decoder</tag>
        <tag>deconvolution</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Gated Feedback Refinement Network for Dense Image Labeling]]></title>
    <url>%2F2018%2F05%2F31%2FGated-Feedback-Refinement-Network-for-Dense-Image-Labeling%2F</url>
    <content type="text"><![CDATA[在本文中作者在encoder-decoder结构的Skip Connection中利用门控机制结合深层特征减少浅层特征得歧义性。 Paper: http://openaccess.thecvf.com/content_cvpr_2017/html/Islam_Gated_Feedback_Refinement_CVPR_2017_paper.html Code: https://github.com/mrochan/gfrnet Gated Feedback Reﬁnement Network作者认为在encoder-decoder结构中直接利用Skip Connection将encoder中的特征传到decoder中是有问题的，因为encoder中的特征相对较浅，具有一定的歧义性，更深层的特征由于有较大的感受野更具有判别能力(如上图)，因此为了减少这种歧义性，利用gating mechanism将浅层特征和深层特征结合再通过Skip Connection传到decoder中。作者所提出的Gated Feedback Reﬁnement Network(G-FRNet)如图所示，encoder网络基于VGG-16，移除了最后的softmax层和全连接层，在其后又加了两个卷积层conv6和conv7，这样对于一幅输入图像就获得了7个大小的特征图($f_1,f_2,…,f_7$)。decoder网络为作者提出的Feedback Reﬁnement Network(FRN)，在decoder中通过使用gating mechanism调节由Skip Connection传来的信息。通过对$f_7$应用通道数类别数3×3的卷积得到第一幅粗略的预测图$Pm^G$。之后的预测图以$Pm^{RU_1}$为例，原始的Skip Connection将$f_5$与对$Pm^G$卷积之后的特征图进行concatenate。而作者首先基于$f_5$和更深层的特征图$f_6$通过gate unit得到gated特征图$G_1$(这么做的原因就是上面说的)来过滤掉类别的歧义性。然后与粗略的预测结果$Pm^G$结合得到大分辨率的特征图$Pm^{RU_1}$。重复这个过程得到后续的预测图$Pm^{RU_2}$，$Pm^{RU_3}$，$Pm^{RU_4}$，$Pm^{RU_5}$。 其中gate unit如图所示，分别对$f^i_g$和$f^{i+1}_g$做3×3的卷积使得$f^{i+1}_g$的通道数与$f^i_g$相同。然后上采样两倍得到$f^{i+1}_{g^{‘}}$，这样特征图大小也相同，最后两个特征图逐元素相乘得到$M_f$。 网络中的RU代表Gated Reﬁnement Unit，其结构如图所示。Gated Reﬁnement Unit的输入为粗略的预测图$R_f$和gated特征图$M_f$。首先对gated特征图$M_f$做3×3的卷积得到特征图$m_f$，然后$m_f$与$R_f$concatenate，最后再做一次3×3的卷积得到特征图$R’_f$。上采样两倍后传入下一个Gated Reﬁnement Unit。可视化结果如下 网络最后会得到6个特征图$Pm^G$，$Pm^{RU_1}$，$Pm^{RU_2}$，$Pm^{RU_3}$，$Pm^{RU_4}$，$Pm^{RU_5}$，每个特征图都可以计算loss，最终的loss fuction由这6个loss求和得到。 Experiments实验平台：caffeGPU：Titan x网络conv1到conv5用VGG-16初始化，其他卷积层Xavier初始化输入图像大小: Pascal VOC 320×320 CamVid 360×480]]></content>
      <categories>
        <category>Semantic Segmentation</category>
      </categories>
      <tags>
        <tag>Semantic Segmentation</tag>
        <tag>encoder-decoder</tag>
        <tag>gating mechanism</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[DeepLab V3--Rethinking Atrous Convolution for Semantic Image Segmentation]]></title>
    <url>%2F2018%2F05%2F30%2FDeepLab-V3-Rethinking-Atrous-Convolution-for-Semantic-Image-Segmentation%2F</url>
    <content type="text"><![CDATA[在DeepLab的第3个版本中，作者主要通过串联或并行Dilation Convolution解决多尺度的问题，并且优化了第2版中提出的Atrous Spatial Pyramid Pooling module，在PASCAL VOC 2012数据集上达到state-of-art的效果。 Paper: https://arxiv.org/abs/1706.05587 Code: https://github.com/tensorflow/models/tree/master/research/deeplab 1. Introduction在这篇论文中，作者提出语义分割的挑战有2个(在上一个版本的论文中作者认为有3个，第三个是由于DXNN的invariance而带来的定位精度的减少，当时的解决方案是CRF)： 由于连续的pooling操作和stride不为1的卷积所造成的特征图的大小不断减小，但是这样却可以让DCNN学到更抽象的特征表达，解决方案是使用dilated Convolution。 另一个挑战是由于物体的多尺度问题。 作者分析了现有的解决多尺度问题的解决方法： 输入多个尺度的图像，分别训练，最后融合预测结果 encoder-decoder结构利用来自于encoder部分的多尺度特征来恢复decoder部分的空间分辨率 在网络后增加额外的模块来捕获大范围的信息，如DenseCRF 采用Spatial Pyramid Pooling 作者采用了串联或者并行的Atrous Spatial Pyramid Pooling module来克服多尺度的问题，设计了DeepLab V3。 2. Related Work在这里作者主要讨论了4种类型的FCN。 Image pyramid这种网络用小尺度的输入来编码大范围的上下文信息，而大尺度的输入用来保留小物体的细节。这种网络的缺点为：对于很大或者很深的网络由于GPU内存的限制并不能很好的同时输入多个尺度的图像进行训练，因此它经常用于测试阶段。 Encoder-decoder这种网络由两部分组成：在encoder中随着特征图大小的逐渐减少，越容易捕获大范围的信息，在decoder中逐渐地恢复特征图的大小。如SegNet，UNet，RefineNet等。 Context module这种网络包含额外的模块来编码大范围的上下文信息。例如DenseCRF. Spatial pyramid pooling这种模型应用金字塔池化在几个范围内来捕获上下文信息。如ParseNet，DeepLab V2，PSPNet。 在本文中，作者则用dilated convolution作为上下文模块和工具进行空间金字塔池化。 3. Methods作者首先以串联方式设计artous convolution模块。在ResNet的最后一个模块叫做block4，在block4后复制block4 3次得到block5，block6，block7。每个block中都包含3个3×3的卷积，除了在最后一个block stride为1，剩余blockstride都为2。如图中(a)所示。这种连续stride使得更深的block可以捕获大范围的信息，但是对于语义分割来说是有害的，因为细节都丢失了。所以在block3开始用artous convolution，rate依次为2，4，8，16。 3.1 Multi-grid MethodMulti-grid Method是指在block4到block7中采用不同的atrous rate。这是通过串联dilation convolution解决多尺度的问题。 3.2 Atrous Spatial Pyramid Pooling作者发现当在3×3的卷积采用越来越大的atrous rates的时候，有效的滤波权重越来越小，在极端情况下，退化成一个简单的1×1的卷积。因此为了克服这个问题且整合全局上下文信息，作者采用了图像级特征，即采用全局平均值池化得到1×1的特征图，然后双线性插值到需要的分辨率。最后改进版的ASPP由两部分组成：(a) 1个1×1的卷积和3个3×3的rate为(6,12,18)的卷积(全部都有batch normalization)(b) 全局平均值池化 所有分支之后concatenate传到另一个1×1的卷积(有batch normalization)，最后传到最终的1×1的卷积。 4. Experimental Evaluation实验设置：学习率策略：polycrop size：513上采样预测结果：在之前地工作中，训练时都是将ground truth下采样，但是作者发现下采样ground truth会移除一些细小地标注从而导致细节没有反向传播，因此保持ground truth不动非常重要，因此训练时采用上采样最终特征图。数据增强：随即缩放，随机左右翻转。 以串联方式应用artous convolution首先对于不同地output stride进行了比较，可以看到随着特征图地不断减小，导致语义分割地结果会持续下降：在保持output stride为16时，在RenNet50和ResNet101中添加不同数量串联block地比较在保持output stride为16时，在ResNet101中添加不同数量串联block和多种rate地artous卷积地比较在训练时output stride为16，但是在测试时output stride变为8可以得到更好地精度，当采用了多尺度地输入和翻转输入时，精度进一步提升。 以并行方式应用artous convolution，即Atrous Spatial Pyramid Pooling]]></content>
      <categories>
        <category>Semantic Segmentation</category>
      </categories>
      <tags>
        <tag>Semantic Segmentation</tag>
        <tag>Dilation Convolution</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Dilated Residual Networks]]></title>
    <url>%2F2018%2F05%2F30%2FDilated-Residual-Networks%2F</url>
    <content type="text"><![CDATA[将Dilation Convolution引入Image Classification任务中，使得在最终的特征图不至于太小而丢失太多的细节。为了由于引入Dilation Convolution带来的网格效应，提出了移除网格效应的方法。除此之外DRN还可以用于object localization和semantic segmentation任务中。 Paper: https://arxiv.org/abs/1705.09914 http://openaccess.thecvf.com/content_cvpr_2017/html/Yu_Dilated_Residual_Networks_CVPR_2017_paper.html Code: https://github.com/fyu/drn 1. Dilated Residual Networks如何将ResNet转变为DRN呢？ 将group 4的第一层stride变为1，dilation变为1，group 4中剩余卷积层和group 5的第一层dilation变为2，group 5中剩余卷积层dilation变为4。 2. Localization 如何将DRN用于Localization呢？ 直接移除global average pooling层，之后用1×1的卷积激活函数为softmax生成n个通道特征图。每一层中的像素值对应每类物体出现的概率。这样没有增加参数，也不需要重新训练模型用于Localization，可以直接将模型用于Localization。 3. Degridding由于引入Dilation Convolution带来的网格效应，作者通过3个步骤逐渐移除网格效应。 Removing max pooling。作者发现网络中的max pooling会带来高频的激活，这种高频的激活会传导到后面的层，最后加剧网格效应。因此取代max pooling层为卷积层。具体为：第一层7×7的卷积核数量由64变为16，然后跟随两个residual block。下图显示了这一操作的影响：DRN-A-18为max pooling之后的特征图，DRN-B-26为将max pooling变为卷积层后的特征图。 Adding layers。在网络的最后逐渐减少dilation，增加了dilation为2的residual block和dilation为1的block。这样ResNet-18由于前两步的操作增加了6层，变为DRN-B-26。 Removing residual connections。由于在上一步中最后加入的residual block含有residual connections，这仍然会带来网格效应，因此需要将level 7和level 8的residual connections移除，这就是最后的DRN-C-26。实验表明：DRN-C-26具有和DRN-A-34相似的精度，比DRN-A-50还高的定位精度和语义分割精度。 4. Experiments4.1 Image Classification 4.2 Weakly-supervised Object Localization 4.3 Semantic Segmentation]]></content>
      <categories>
        <category>Semantic Segmentation</category>
      </categories>
      <tags>
        <tag>Semantic Segmentation</tag>
        <tag>Dilation Convolution</tag>
        <tag>Image Classification</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Pixel Deconvolutional Networks]]></title>
    <url>%2F2018%2F05%2F30%2FPixel-Deconvolutional-Networks%2F</url>
    <content type="text"><![CDATA[作者为了解决deconvolution中出现的棋盘格效应，提出了pixel deconvolutional layer(PixelDCL)，来直接建立上采样特征图相邻像素的关系，可以直接替换掉任何deconvolution layer。 Paper: https://arxiv.org/abs/1705.06820 Code: https://github.com/divelab/PixelDCN 棋盘格效应： 效果图： 第三行为使用deconvolution的结果，第四行为作者提出的PixelDCL的结果。 1. Deconvolution layer1维的deconvolution：输入特征图为4×1，输出特征图为8×1。输入特征图的每个像素与卷积核相乘，结果依次偏移两个值，结果特征图的像素值为每列求和。可以看到紫色像素只和(1,3)有关，橙色像素只和(2,4)有关。因此可以分解为两个独立的卷积，如右图所示。可以看出相邻像素没有直接的关系，这就导致了棋盘格效应。 2维的deconvolution：对于2维的deconvolution，同理输入特征图为4×4，输出特征图为8×8，中间由不同的卷积核得到4个特征图，最终输出特征图由4个中间特征图重新排列得到。同样相邻像素没有直接的关系，这就导致了棋盘格效应。 2. Pixel deconvolutional layer在作者提出的PixelDCL中，中间特征图图是依次生成的，只有第一个中间特征图和输入特征图有关，后续所有中间特征图都只和之前的特征图有关。最终输出特征图还是由4个中间特征图重新排列得到。实际上，作者还提出了一种input pixel deconvolutional layer (iPixelDCL)，中间特征图不仅和之前的特征图有关，还和输入特征图有关，但是实验发现PixelDCL的效果比iPixelDCL和deconvolution效果好。 PixelDCL可以应用于语义分割网络，VAE和GAN中，作者将PixelDCL应用于UNet和VAE中测试了PixelDCL的效果。实际中，作者设计了一个更加简单的PixelDCL。如下图所示： 3. 实验结果]]></content>
      <categories>
        <category>Semantic Segmentation</category>
      </categories>
      <tags>
        <tag>Semantic Segmentation</tag>
        <tag>deconvolution</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ICNet for Real-Time Semantic Segmentation on High-Resolution Images]]></title>
    <url>%2F2018%2F05%2F29%2FICNet-for-Real-Time-Semantic-Segmentation-on-High-Resolution-Images%2F</url>
    <content type="text"><![CDATA[project：https://hszhao.github.io/projects/icnet/ Paper: https://arxiv.org/abs/1704.08545 Code: https://github.com/hszhao/ICNet 1.Introduction当前fast semantic segmentation的状态下图是在Cityscapes数据集上测试速度和mIoU的比较： 一方面，可以看到大多数现有方法几乎都不能达到实时的要求。另一方面，虽然SegNet，ENet，SQ速度较快，但是mIoU低于60%。因此作者提出建立一个实用的快速语义分割框架。核心思想是：让低分辨率的图像先通过网络得到粗略的预测图，然后所提出的cascade fusion unit引入中等分辨率和高分辨率的图像逐渐改善预测图。 主要贡献 提出了image cascade network (ICNet)，它可以同时有效地利用低分辨率地语义信息和高分辨率的细节。 所提出的ICNet实现了5倍多的加速，减少了5倍多的内存消耗。 可以在1024×2048的图像上达到30.3fps的同时实现高质量的结果。 2. Related Work分别对High Quality Semantic Segmentation，Fast Semantic Segmentation和Video Segmentation Architectures进行回顾。 3. Speed Analysis3.1. Time Budget在这里作者先回顾了PSPNet的分割性能，然后引入了加速语义分割的直观策略，从它们网络的去缺点中，描述所提出的image cascade框架和cascade feature fusion unit。下图展示了PSPNet50(这里是优化后的，详细的变化在原文第6节)在512×1024和1024×2048分辨率下的运行时间。很显然当图像变大时，运行时间必然增加。同时网络的宽度(或卷积核的数量)也会影响运行时间。如在stage4 和 stage5阶段有相同的空间分辨率，但是在stage5的时间是stage4的4倍多。这是因为在stage5中的卷积核的数量是stage4的两倍。 3.2. Intuitive Speedup这里主要有三点： 1.输入降采样后的图像具体做法是：输入1/2或1/4的降采样后的图像，得到预测结果后，再上采样到原始大小。这样做的缺点是虽然时间减少了但是预测的结果非常粗糙，丢失了许多小的但是却重要的细节。如下图： 2.除了直接降采样图像外，另一个直接的方法是降采样特征图。在FCN中降采样32倍，DeepLab中降采样了8倍。作者测试PSPNet50降采样8倍，16倍，32倍的结果如下： 可以看到小的特征图需要的时间更少，而且即使是最小的特征图仍然需要131ms，达不到实时的要求。 3.除了以上策略外，另一个自然的方式是模型压缩。作者测试了最近提出的一种压缩方法，发现并没有满足实时的要求。即使只保留1/4的卷积核，所需要的时间还是太长了，与此同时mIuO非常低，不能产生合理的分割图。 4. Image Cascade Network4.1 主要结构和分支 首先将输入图像缩放到原图的1/2和1/4，分别将3个尺度的图像输入网络的3个分支。 对于低分辨率的图像，即原始图像的1/4大小，通过卷积及池化后缩小了1/8，对应原图的1/32，然后应用dilated convlution来增加感受野，输出为原始图像大小的1/32。 对于中等分辨率的图像，即原始图像的1/2大小，同样通过几个卷积层和池化层(和上面一个分支共享一部分卷积层来减少参数数量)后缩小了8倍，即原始图像的1/16。为了融合1/32的特征图和1/8的特征图，作者提出了cascade feature fusion(CFF)单元来产生1/16的特征图。 对于高等分辨率的图像，即原始图像，同样通过几个卷积层和池化层后缩小了8倍，得到1/8的特征图。为了融合1/32的特征图和1/8的特征图，作者提出了cascade feature fusion(CFF)单元来产生1/16的特征图。由于在中等分辨率时，已经恢复了大多数在低分辨率中丢失的语义信息，因此可以限制处理高分辨率时的卷积层的数量。只使用了3×3的卷积核和stride为2来降采样到原图的1/8。然后使用CFF单元整合由中等分辨率得到的特征图和原始图片得到的特征图。最终得到原始图像的1/8的特征图。 Cascade Label Guidance为了辅助学习过程，作者提出了Cascade Label Guidance策略：在训练时每个分支上采样2倍后，同时也将ground truth降采样1/16，1/8和1/4(紫线)，这样损失函数就有3项。在测试时将低分辨率和中等分辨率的这一操作丢弃。Cascade Label Guidance策略可以保证训练迭代更容易，梯度优化更平滑。 4.2 分支分析在ICnet中在第二分支有17个卷积层，第三分支只有3个卷积层，而且第一分支和第二分支共享部分计算。最深的网络结构应用在低分辨率图像上，这可以有效的提取大多数语义信息。即使超过50层，测试时运行时间不超过18ms，内存消耗不超过0.6G。因此，所提出的ICNet是一个非常高效和节省内存的结构。 4.3 和其他Cascade Structures的不同之处其他Cascade Structures都聚焦于融合单尺度或多尺度的输入的不同层的特征，而ICNet则用低分辨率的图像通过主体语义分割分支，再用高分辨率信息进行优化。 5. Cascade Feature Fusion and Final Model 为了结合不同分辨率的图像，提出了Cascade Feature Fusion(CCF)单元，输入包含3个部分：两个特征图$F_1$和$F_2$，大小为$H_1\times{W_1\times{C_1}}$和$H_2\times{W_2\times{C_2}}$,和一个ground truth label大小为$H_2\times{W_2\times{1}}$。$F_2$的大小是$F_1$的2倍。首先将$F_1$上采样2倍使得与$F_2$大小相同，然后用$3\times3$的卷积核，dilation 1的dilated convolution对上采样的特征图refine。而对于特征图$F_2$用$1\times1$的卷积使得与特征图$F_1$具有相同的通道数。在dilated convolution和$1\times1$的卷积后都使用了Batch normalization。最后两个特征图相加再通过ReLU激活函数，得到融合的特征图F^{‘}_2。除此之外，还对对上采样的特征图$F_1$添加了辅助分类器，辅助分类器损失的权重设为0.4。 5.1 The Loss Function最终的损失函数由3部分组成：$$L=\lambda_{1}L_1+\lambda_{2}L_2+\lambda_{3}L_3$$ 5.2 Final Model Compression最后为了进一步减少运行时间，对模型进行压缩。作者采取的是渐进式的方式。以压缩率为1/2为例，并没有直接移除一半的卷积核，而是先保存3/4的卷积核再进行fine-tuning。之后再移除更多的卷积核，再fine-tuning直到达到1/2的压缩率。 如何选择哪个卷积核要被移除呢？ 作者通过计算卷积核的L1范数，再根据L1范数进行排序，去掉那些较小的卷积核。这种方式不会剧烈的更新所有参数，因此可以较好地减少网络大小。 6. Experimental Evaluation实验平台：caffe CUDA7.5 CUDNN V5 GPU：TitanX 基础网络：PSPNet 修改： 在金字塔池化模块时的concat操作变为sum，减少特征维度从4096到2048 改变金字塔池化后的卷积操作的卷积核大小由$3\times3$变为$1\times1$ batch size：16 base learning rate：0.01 learning policy：poly max iteration：30k momentum：0.9 weight decy：0.0001 数据增强：随机镜像和随机缩放到0.5到2倍之间。 6.1 Model Compression以PSPNet50为baseline使用相同的模型压缩方法，与ICNet比较。 6.2 Ablation Study for Image Cascade Framework Video example：]]></content>
      <categories>
        <category>Semantic Segmentation</category>
      </categories>
      <tags>
        <tag>Semantic Segmentation</tag>
        <tag>Real-Time</tag>
      </tags>
  </entry>
</search>
