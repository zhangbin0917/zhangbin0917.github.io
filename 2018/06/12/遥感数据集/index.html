<!DOCTYPE html>



  


<html class="theme-next gemini use-motion" lang="">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">



  
  
    
    
  <script src="/lib/pace/pace.min.js?v=1.0.2"></script>
  <link href="/lib/pace/pace-theme-minimal.min.css?v=1.0.2" rel="stylesheet">







<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT" />










<meta name="description" content="整理了遥感中的数据集，长期更新！！">
<meta name="keywords" content="Remote Sensing, Image Processing, Deep learning">
<meta property="og:type" content="article">
<meta property="og:title" content="遥感数据集">
<meta property="og:url" content="http://yoursite.com/2018/06/12/遥感数据集/index.html">
<meta property="og:site_name" content="Zhang Bin&#39;s Blog">
<meta property="og:description" content="整理了遥感中的数据集，长期更新！！">
<meta property="og:locale" content="default">
<meta property="og:image" content="http://csc.lsu.edu/~saikat/deepsat/images/sat_img.png">
<meta property="og:image" content="https://captain-whu.github.io/AID/aid-dataset.png">
<meta property="og:image" content="http://www.escience.cn/system/img?imgId=86962">
<meta property="og:image" content="https://lh3.googleusercontent.com/SsMQOa7cpDEMb4KUG9i3qKlaEAXcVoB-f3oARLcw7kYRtkHoKljMh1e8VllmnXTpEJoPaLnzghaOvrcDMf0BAGHXY0hsZxApEBfopa8AghlXc_qyhW4=w773">
<meta property="og:image" content="https://github.com/wzx918/test/raw/master/%E6%95%B0%E9%87%8F%E5%88%86%E5%B8%83.png">
<meta property="og:image" content="https://github.com/wzx918/test/raw/master/128%E6%A0%B7%E6%9C%AC%E5%9B%BE.png">
<meta property="og:image" content="https://gdo152.llnl.gov/cowc/COWC_Logo.png">
<meta property="og:image" content="https://captain-whu.github.io/DOTA/instances-DOTA.jpg">
<meta property="og:image" content="http://xviewdataset.org/img/example6.jpg">
<meta property="og:image" content="http://challenge.xviewdataset.org/static/example_labeled.jpg">
<meta property="og:image" content="https://sites.google.com/site/michelevolpiresearch/_/rsrc/1438622321150/data/zurich-dataset/zh8_web.png?height=206&width=320">
<meta property="og:image" content="https://sites.google.com/site/michelevolpiresearch/_/rsrc/1438622318886/data/zurich-dataset/zh8_gt_web.png?height=206&width=320">
<meta property="og:image" content="http://www2.isprs.org/tl_files/isprs/wg34/images/overview_tiles.jpg">
<meta property="og:image" content="http://www2.isprs.org/tl_files/isprs/wg34/images/tile_overview.resized.png">
<meta property="og:image" content="http://www.grss-ieee.org/wp-content/uploads/2016/12/banner.png">
<meta property="og:image" content="https://i.imgur.com/70AzK8b.png">
<meta property="og:image" content="http://www.grss-ieee.org/wp-content/uploads/2018/01/dfc2018_header_v4.png">
<meta property="og:image" content="http://deepglobe.org/uploads/1/1/7/0/117046351/published/screen-shot-2018-02-16-at-4-08-32-pm.png?1518826243">
<meta property="og:image" content="https://project.inria.fr/aerialimagelabeling/files/2011/12/chi1-300x300.jpg">
<meta property="og:image" content="https://project.inria.fr/aerialimagelabeling/files/2011/12/chi2-300x300.jpg">
<meta property="og:image" content="http://deepglobe.org/uploads/1/1/7/0/117046351/published/picture1.png?1518826212">
<meta property="og:image" content="https://crowdai-shared.s3.eu-central-1.amazonaws.com/markdown_editor/mapping_challenge_1.jpg">
<meta property="og:image" content="http://study.rsgis.whu.edu.cn/pages/download/instruction.files/image001.jpg">
<meta property="og:image" content="http://study.rsgis.whu.edu.cn/pages/download/instruction.files/image002.jpg">
<meta property="og:image" content="http://study.rsgis.whu.edu.cn/pages/download/instruction.files/image003.png">
<meta property="og:image" content="http://study.rsgis.whu.edu.cn/pages/download/instruction.files/image004.jpg">
<meta property="og:image" content="https://blog.werobotics.org/wp-content/uploads/2018/08/zmi-geonode.png">
<meta property="og:image" content="http://deepglobe.org/uploads/1/1/7/0/117046351/published/pred.png?1518826209">
<meta property="og:image" content="https://www.onera.fr/sites/default/files/300/beirut-triptych.png">
<meta property="og:image" content="https://github.com/201528014227051/RSICD_optimal/raw/master/example.PNG">
<meta property="og:image" content="https://s3-us-west-2.amazonaws.com/codalab-webiks/Images/examples.jpg">
<meta property="og:updated_time" content="2018-09-15T14:41:38.496Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="遥感数据集">
<meta name="twitter:description" content="整理了遥感中的数据集，长期更新！！">
<meta name="twitter:image" content="http://csc.lsu.edu/~saikat/deepsat/images/sat_img.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Gemini',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":true,"scrollpercent":true,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/2018/06/12/遥感数据集/"/>





  <title>遥感数据集 | Zhang Bin's Blog</title>
  





  <script type="text/javascript">
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?b50a5b1e67dd059ba3990c68a8ba3110";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script>




</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="default">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Zhang Bin's Blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">Less interests, more interest!!!</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            Categories
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            Tags
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            About
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br />
            
            Search
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off"
             placeholder="Searching..." spellcheck="false"
             type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/06/12/遥感数据集/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Bin Zhang">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/jonathon-reed-190894.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Zhang Bin's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">遥感数据集</h1>
        

        <div class="post-meta">
		  
            <i class="fa fa-thumb-tack"></i>
            <font color=7D26CD>置顶</font>
            <span class="post-meta-divider">|</span>
          
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-06-12T12:43:15+08:00">
                2018-06-12
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Dataset/" itemprop="url" rel="index">
                    <span itemprop="name">Dataset</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          
            <div class="post-wordcount">
              
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Words count in article&#58;</span>
                
                <span title="Words count in article">
                  9,600
                </span>
              

              

              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>整理了遥感中的数据集，长期更新！！<br><a id="more"></a></p>
<h1 id="场景分类"><a href="#场景分类" class="headerlink" title="场景分类"></a>场景分类</h1><h2 id="1-UC-Merced-Land-Use-Dataset"><a href="#1-UC-Merced-Land-Use-Dataset" class="headerlink" title="1. UC Merced Land Use Dataset"></a>1. UC Merced Land Use Dataset</h2><p><a href="http://vision.ucmerced.edu/datasets/landuse.html" target="_blank" rel="noopener">http://vision.ucmerced.edu/datasets/landuse.html</a><br>This is a <strong>21 class</strong> land use image dataset meant for research purposes.<br>There are <strong>100 images</strong> for each of the following classes:<br><font color="red">agricultural，airplane，baseballdiamond，beach，buildings，chaparral，denseresidential，forest，freeway，golfcourse，harbor，intersection，mediumresidential，mobilehomepark，overpass，parkinglot，river，runway，sparseresidential，storagetanks，tenniscourt</font><br>Each image measures <strong>256x256</strong> pixels.<br>The images were manually extracted from large images from the USGS National Map Urban Area Imagery collection for various urban areas around the country. The pixel resolution of this public domain imagery is <strong>1 foot</strong>.<br><strong>Reference：</strong></p>
<ul>
<li><em>Yi Yang and Shawn Newsam, “Bag-Of-Visual-Words and Spatial Extensions for Land-Use Classification,” ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems (ACM GIS), 2010.</em></li>
</ul>
<h2 id="2-WHU-RS19"><a href="#2-WHU-RS19" class="headerlink" title="2. WHU-RS19"></a>2. WHU-RS19</h2><p><a href="http://captain.whu.edu.cn/repository.html" target="_blank" rel="noopener">http://captain.whu.edu.cn/repository.html</a><br><a href="http://captain.whu.edu.cn/datasets/WHU-RS19.zip" target="_blank" rel="noopener">http://captain.whu.edu.cn/datasets/WHU-RS19.zip</a><br>WHU-RS19是从谷歌卫星影像上获取<strong>19类</strong>遥感影像，可用于场景分类和检索。<br><strong>Reference：</strong></p>
<ul>
<li><em>G.-S. Xia, W. Yang, J. Delon, Y. Gousseau. H. Maitre, H. Sun, “Structural high-resolution satellite image indexing”. Symposium: 100 Years ISPRS - Advancing Remote Sensing Science: Vienna, Austria, 2010</em></li>
</ul>
<h2 id="3-RSSCN7"><a href="#3-RSSCN7" class="headerlink" title="3. RSSCN7"></a>3. RSSCN7</h2><p><a href="https://sites.google.com/site/qinzoucn/documents" target="_blank" rel="noopener">https://sites.google.com/site/qinzoucn/documents</a><br>This dataset contains <strong>2800</strong> remote sensing images which are from <strong>7 typical scene categories</strong> - <font color="red">the grass land, forest, farm land, parking lot, residential region, industrial region, and river&amp;lake</font>. For each category, there are <strong>400</strong> images collected from the Google Earth which are sampled on <strong>4 different scales</strong> with <strong>100 images per scale</strong>. Each image has a size of <strong>400*400</strong> pixels. This dataset is rather challenging due to the wide diversity of the scene images which are captured under changing seasons and varying weathers, and sampled with different scales.<br><strong>Reference：</strong></p>
<ul>
<li><em>Qin Zou, Lihao Ni, Tong Zhang and Qian Wang, Deep learning based feature selection for remote sensing scene classification, IEEE Geoscience and Remote Sensing Letters, vol. 12, no. 11, pp.2321-2325, 2015.</em></li>
</ul>
<h2 id="4-SAT-4-and-SAT-6-airborne-datasets"><a href="#4-SAT-4-and-SAT-6-airborne-datasets" class="headerlink" title="4. SAT-4 and SAT-6 airborne datasets"></a>4. SAT-4 and SAT-6 airborne datasets</h2><p><a href="http://csc.lsu.edu/~saikat/deepsat/" target="_blank" rel="noopener">http://csc.lsu.edu/~saikat/deepsat/</a><br><a href="https://arxiv.org/abs/1509.03602" target="_blank" rel="noopener">https://arxiv.org/abs/1509.03602</a><br><img src="http://csc.lsu.edu/~saikat/deepsat/images/sat_img.png" alt=""><br>Images were extracted from the National Agriculture Imagery Program (NAIP) dataset. The NAIP dataset consists of a total of 330,000 scenes spanning the whole of the Continental United States (CONUS). We used the uncompressed digital Ortho quarter quad tiles (DOQQs) which are GeoTIFF images and the area corresponds to the United States Geological Survey (USGS) topographic quadrangles. The average image tiles are ~6000 pixels in width and ~7000 pixels in height, measuring around 200 megabytes each. The entire NAIP dataset for CONUS is ~65 terabytes. The imagery is acquired at a <strong>1-m ground sample distance (GSD)</strong> with a horizontal accuracy that lies within six meters of photo-identifiable ground control points. The images consist of 4 bands - red, green, blue and Near Infrared (NIR). In order to maintain the high variance inherent in the entire NAIP dataset, we sample image patches from a multitude of scenes (a total of 1500 image tiles) covering different landscapes like rural areas, urban areas, densely forested, mountainous terrain, small to large water bodies, agricultural areas, etc. covering the whole state of California. An image labeling tool developed as part of this study was used to manually label uniform image patches belonging to a particular landcover class. Once labeled, 28x28 non-overlapping sliding window blocks were extracted from the uniform image patch and saved to the dataset with the corresponding label. We chose 28x28 as the window size to maintain a significantly bigger context, and at the same time not to make it as big as to drop the relative statistical properties of the target class conditional distributions within the contextual window. Care was taken to avoid interclass overlaps within a selected and labeled image patch.<br>The datasets are encoded as MATLAB .mat files that can be read using the standard load command in MATLAB. Each sample image is <strong>28x28</strong> pixels and consists of <strong>4 bands - red, green, blue and near infrared</strong>. The training and test labels are 1x4 and 1x6 vectors for SAT-4 and SAT-6 respectively having a single 1 indexing a particular class from 0 through 4 or 6 and 0 values at all other indices.<br><strong>Reference:</strong></p>
<ul>
<li><em>Saikat Basu, Sangram Ganguly, Supratik Mukhopadhyay, Robert Dibiano, Manohar Karki and Ramakrishna Nemani, DeepSat - A Learning framework for Satellite Imagery, ACM SIGSPATIAL 2015.</em></li>
</ul>
<h2 id="5-RSC11"><a href="#5-RSC11" class="headerlink" title="5. RSC11"></a>5. RSC11</h2><p><a href="https://www.researchgate.net/publication/271647282_RS_C11_Database6" target="_blank" rel="noopener">https://www.researchgate.net/publication/271647282_RS_C11_Database6</a><br><strong>11 scenes</strong>, all using high resolution remote sensing images, downloaded from Google Earth</p>
<h2 id="6-SIRI-WHU"><a href="#6-SIRI-WHU" class="headerlink" title="6. SIRI-WHU"></a>6. SIRI-WHU</h2><p><a href="http://www.lmars.whu.edu.cn/prof_web/zhongyanfei/e-code.html" target="_blank" rel="noopener">http://www.lmars.whu.edu.cn/prof_web/zhongyanfei/e-code.html</a><br>This is a <strong>12-class</strong> Google image dataset of SIRI-WHU meant for research purposes.<br>There are <strong>200 images</strong> for each of the following classes:<br><font color="red">Agriculture, Commercial, Harbor, Idle land, Industrial, Meadow, Overpass, Park, Pond, Residential, River, Water</font><br>Each image measures <strong>200*200 pixels</strong>, with a <strong>2-m spatial resolution</strong>.<br>This dataset was acquired from Google Earth (Google Inc.) and mainly covers urban areas in China, and the scene image dataset is designed by RS_IDEA Group in Wuhan University (SIRI-WHU).<br><strong>Reference:</strong></p>
<ul>
<li><em>B. Zhao, Y. Zhong, G.-s. Xia, and L. Zhang, “Dirichlet-Derived Multiple Topic Scene Classification Model Fusing Heterogeneous Features for High Spatial Resolution Remote Sensing Imagery,” IEEE Transactions on Geoscience and Remote Sensing, vol. 54, no. 4, pp. 2108-2123, Apr. 2016.</em></li>
<li><em>B. Zhao, Y. Zhong, L. Zhang, and B. Huang, “The Fisher Kernel Coding Framework for High Spatial Resolution Scene Classification,” Remote Sensing, vol. 8, no. 2, p. 157, doi:10.3390/rs8020157 2016.</em></li>
<li><em>Q. Zhu, Y. Zhong, B. Zhao, G.-S. Xia, and L. Zhang, “Bag-of-Visual-Words Scene Classifier with Local and Global Features for High Spatial Resolution Remote Sensing Imagery,” IEEE Geoscience and Remote Sensing Letters, DOI:10.1109/LGRS.2015.2513443 2016.</em></li>
</ul>
<h2 id="7-AID"><a href="#7-AID" class="headerlink" title="7. AID"></a>7. AID</h2><p><a href="http://www.lmars.whu.edu.cn/xia/AID-project.html" target="_blank" rel="noopener">http://www.lmars.whu.edu.cn/xia/AID-project.html</a><br><a href="https://captain-whu.github.io/AID" target="_blank" rel="noopener">https://captain-whu.github.io/AID</a><br><img src="https://captain-whu.github.io/AID/aid-dataset.png" alt=""><br>AID is a new large-scale aerial image dataset, by collecting sample images from Google Earth imagery. Note that although the Google Earth images are post-processed using RGB renderings from the original optical aerial images, it has proven that there is no significant difference between the Google Earth images with the real optical aerial images even in the pixel-level land use/cover mapping. Thus, the Google Earth images can also be used as aerial images for evaluating scene classification algorithms.<br>The new dataset is made up of the following <strong>30 aerial scene types</strong>: <font color="red">Aairport, bare land, baseball field, beach, bridge, center, church, commercial, dense residential, desert, farmland, forest, industrial, meadow, medium residential, mountain, park, parking, playground, pond, port, railway station, resort, river, school, sparse residential, square, stadium, storage tanks and viaduct</font>. All the images are labelled by the specialists in the field of remote sensing image interpretation, and some samples of each class are shown in Fig. In all, the AID dataset has a number of <strong>10000 images</strong> within 30 classes.<br><strong>Reference:</strong></p>
<ul>
<li><em>G.-S. Xia, J. Hu, F. Hu, B. Shi, X. Bai, Y. Zhong, L. Zhang, X. Lu, “AID: A benchmark dataset for performance evaluation of aerial scene classification”, IEEE Transactions on Geoscience and Remote Sensing, vol. 55, no. 7, pp. 3965-3981, 2017.</em></li>
</ul>
<h2 id="8-NWPU-RESISC45"><a href="#8-NWPU-RESISC45" class="headerlink" title="8. NWPU-RESISC45"></a>8. NWPU-RESISC45</h2><p><a href="http://www.escience.cn/people/JunweiHan/NWPU-RESISC45.html" target="_blank" rel="noopener">http://www.escience.cn/people/JunweiHan/NWPU-RESISC45.html</a><br><a href="https://arxiv.org/abs/1703.00121" target="_blank" rel="noopener">https://arxiv.org/abs/1703.00121</a><br><img src="http://www.escience.cn/system/img?imgId=86962" alt=""><br>NWPU-RESISC45 dataset is a publicly available benchmark for REmote Sensing Image Scene Classification (RESISC), created by Northwestern Polytechnical University (NWPU). This dataset contains <strong>31,500 images</strong>, covering <strong>45 scene classes</strong> with <strong>700 images in each class</strong>. These 45 scene classes include <font color="red">airplane, airport, baseball diamond, basketball court, beach, bridge, chaparral, church, circular farmland, cloud, commercial area, dense residential, desert, forest, freeway, golf course, ground track field, harbor, industrial area, intersection, island, lake, meadow, medium residential, mobile home park, mountain, overpass, palace, parking lot, railway, railway station, rectangular farmland, river, roundabout, runway, seaice, ship, snowberg, sparse residential, stadium, storage tank, tennis court, terrace, thermal power station, and wetland</font>.<br><strong>Reference:</strong></p>
<ul>
<li><em>G. Cheng, J. Han, X. Lu. Remote Sensing Image Scene Classification: Benchmark and State of the Art. Proceedings of the IEEE.</em></li>
</ul>
<h2 id="9-PatternNet"><a href="#9-PatternNet" class="headerlink" title="9. PatternNet"></a>9. PatternNet</h2><p><a href="https://sites.google.com/view/zhouwx/dataset" target="_blank" rel="noopener">https://sites.google.com/view/zhouwx/dataset</a><br><img src="https://lh3.googleusercontent.com/SsMQOa7cpDEMb4KUG9i3qKlaEAXcVoB-f3oARLcw7kYRtkHoKljMh1e8VllmnXTpEJoPaLnzghaOvrcDMf0BAGHXY0hsZxApEBfopa8AghlXc_qyhW4=w773" alt=""><br>PatternNet is a large-scale high-resolution remote sensing dataset collected for <strong>remote sensing image retrieval</strong>. There are <strong>38 classes</strong> and each class has <strong>800 images</strong> of size <strong>256×256 pixels</strong>. The images in PatternNet are collected from Google Earth imagery or via the Google Map API for some US cities. The figure shows some example images from each class.<br><strong>Reference:</strong></p>
<ul>
<li><em>Zhou, W., Newsam, S., Li, C., &amp; Shao, Z. (2017). PatternNet: A Benchmark Dataset for Performance Evaluation of Remote Sensing Image Retrieval. arXiv preprint arXiv:1706.03424.</em></li>
<li><em>Zhou, W., Newsam, S., Li, C., &amp; Shao, Z. (2017). Learning Low Dimensional Convolutional Neural Networks for High-Resolution Remote Sensing Image Retrieval. Remote Sensing, 9(5), 489. </em></li>
<li><em>Zhou, W., Shao, Z., Diao, C., &amp; Cheng, Q. (2015). High-resolution remote-sensing imagery retrieval using sparse features by auto-encoder. Remote Sensing Letters, 6(10), 775-783.</em> </li>
</ul>
<h2 id="10-RSI-CB"><a href="#10-RSI-CB" class="headerlink" title="10. RSI-CB"></a>10. RSI-CB</h2><p><a href="https://arxiv.org/abs/1705.10450" target="_blank" rel="noopener">https://arxiv.org/abs/1705.10450</a><br><a href="https://github.com/lehaifeng/RSI-CB" target="_blank" rel="noopener">https://github.com/lehaifeng/RSI-CB</a><br><img src="https://github.com/wzx918/test/raw/master/%E6%95%B0%E9%87%8F%E5%88%86%E5%B8%83.png" alt=""><br><img src="https://github.com/wzx918/test/raw/master/128%E6%A0%B7%E6%9C%AC%E5%9B%BE.png" alt=""><br>Considering the different image size requirements of the DCNN, construct <strong>two datasets</strong> of <strong>256 × 256</strong> and <strong>128 × 128</strong> pixel sizes (<strong>RSI-CB256</strong> and <strong>RSI-CB128</strong>, respectively) with <strong>0.3–3-m spatial resolutions</strong>. The former contains <strong>35 categories</strong> and <strong>more than 24,000 images</strong>. The latter contains <strong>45 categories</strong> and <strong>more than 36,000 images</strong>. We establish a strict object category system according to the national standard of land-use classification in China and the hierarchical grading mechanism of ImageNet. The six categories are agricultural land, construction land and facilities, transportation and facilities, water and water conservancy facilities, woodland, and other land.</p>
<h2 id="11-AID"><a href="#11-AID" class="headerlink" title="11. AID++"></a>11. AID++</h2><p><a href="https://arxiv.org/abs/1806.00801" target="_blank" rel="noopener">https://arxiv.org/abs/1806.00801</a><br><strong>Reference:</strong></p>
<ul>
<li><em>AID++：An Updated Version of AID on Scene Classification</em></li>
</ul>
<h1 id="目标检测"><a href="#目标检测" class="headerlink" title="目标检测"></a>目标检测</h1><h2 id="1-RSOD-Dataset"><a href="#1-RSOD-Dataset" class="headerlink" title="1. RSOD-Dataset"></a>1. RSOD-Dataset</h2><p><a href="https://github.com/RSIA-LIESMARS-WHU/RSOD-Dataset-" target="_blank" rel="noopener">https://github.com/RSIA-LIESMARS-WHU/RSOD-Dataset-</a><br>It is an open dataset for object detection in remote sensing images. The dataset includes <font color="red">aircraft, oiltank, playground and overpass</font>.<br>The format of this dataset that for PASCAL VOC.<br>The datase includes 4 files, and each file is for one kind of object. Please download the dataset files from BaiduYun.</p>
<ol>
<li><a href="http://pan.baidu.com/s/1eRWFV5C" target="_blank" rel="noopener">aircraft dataset</a>, <strong>4993</strong> aircrafts in <strong>446</strong> images.</li>
<li><a href="http://pan.baidu.com/s/1nuD4KLb" target="_blank" rel="noopener">playground</a>, <strong>191</strong> playgrounds in <strong>189</strong> images.</li>
<li><a href="http://pan.baidu.com/s/1kVKAFB5" target="_blank" rel="noopener">overpass</a>, <strong>180</strong> overpass in <strong>176</strong> overpass.</li>
<li><a href="http://pan.baidu.com/s/1kUZn4zX" target="_blank" rel="noopener">oiltank</a>, <strong>1586</strong> oiltanks in <strong>165</strong> images.</li>
</ol>
<p><strong>Reference:</strong></p>
<ul>
<li><em>Y. Long, Y. Gong, Z. Xiao and Q. Liu, “Accurate Object Localization in Remote Sensing Images Based on Convolutional Neural Networks,” in IEEE Transactions on Geoscience and Remote Sensing, vol. 55, no. 5, pp. 2486-2498, May 2017. doi: 10.1109/TGRS.2016.2645610, <a href="http://ieeexplore.ieee.org/abstract/document/7827088/" target="_blank" rel="noopener">link</a></em></li>
<li><em>Z Xiao, Q Liu, G Tang, X Zhai, “Elliptic Fourier transformation-based histograms of oriented gradients for rotationally invariant object detection in remote-sensing images”, International Journal of Remote Sensing, vol. 36, no. 2, 2015</em></li>
</ul>
<h2 id="2-NWPU-VHR-10-dataset"><a href="#2-NWPU-VHR-10-dataset" class="headerlink" title="2. NWPU VHR-10 dataset"></a>2. NWPU VHR-10 dataset</h2><p>NWPU VHR-10 dataset is a publicly available <strong>10-class</strong> geospatial object detection dataset used for research purposes only.These ten classes of objects are <font color="red">airplane, ship, storage tank, baseballdiamond, tennis court, basketball court, ground track field, harbor, bridge,and vehicle</font>. This dataset contains totally <strong>800</strong> very-high-resolution (VHR)remote sensing images that were cropped from Google Earth and Vaihingen dataset and then manually annotated by experts.<br>Thisdataset can be downloaded from <a href="https://1drv.ms/u/s!AmgKYzARBl5cczaUNysmiFRH4eE" target="_blank" rel="noopener">OneDrive</a> or <a href="http://pan.baidu.com/s/1hqwzXeG" target="_blank" rel="noopener">BaiduWangpan</a>.<br><strong>Reference:</strong></p>
<ul>
<li><em>G. Cheng, J. Han, P. Zhou, L. Guo. Multi-class geospatial object detection and geographic imageclassification based on collection of part detectors. ISPRS Journal ofPhotogrammetry and Remote Sensing, 98: 119-132, 2014.</em></li>
<li><em>G. Cheng, J. Han. A survey on objectdetection in optical remote sensing images. ISPRS Journal of Photogrammetry andRemote Sensing, 117: 11-28, 2016.</em></li>
<li><em>G. Cheng, P. Zhou, J. Han. Learningrotation-invariant convolutional neural networks for object detection in VHRoptical remote sensing images. IEEE Transactions on Geoscience and RemoteSensing, 54(12): 7405-7415, 2016.</em></li>
</ul>
<h2 id="3-Cars-Overhead-With-Context-COWC"><a href="#3-Cars-Overhead-With-Context-COWC" class="headerlink" title="3. Cars Overhead With Context(COWC)"></a>3. Cars Overhead With Context(COWC)</h2><p><img src="https://gdo152.llnl.gov/cowc/COWC_Logo.png" alt=""><br><a href="https://gdo152.llnl.gov/cowc/" target="_blank" rel="noopener">https://gdo152.llnl.gov/cowc/</a><br><a href="http://gdo-datasci.ucllnl.org/cowc/ECCV.poster.big.jpg" target="_blank" rel="noopener">Poster</a> <a href="http://gdo-datasci.ucllnl.org/cowc/mundhenk_et_al_eccv_2016.pdf" target="_blank" rel="noopener">Paper</a><br>Github: <a href="https://github.com/LLNL/cowc" target="_blank" rel="noopener">https://github.com/LLNL/cowc</a><br>FTP: <a href="ftp://gdo152.ucllnl.org/cowc/" target="_blank" rel="noopener">ftp://gdo152.ucllnl.org/cowc/</a><br>The <strong>Cars Overhead With Context (COWC)</strong> data set is a large set of annotated cars from overhead. It is useful for training a device such as a deep neural network to learn to detect and/or count cars. </p>
<p>The dataset has the following attributes:</p>
<ul>
<li>(1) Data from overhead at <strong>15 cm per pixel resolution</strong> at ground (all data is EO). </li>
<li>(2) Data from <strong>six</strong> distinct locations: Toronto Canada, Selwyn New Zealand, Potsdam and Vaihingen Germany, Columbus and Utah United States. </li>
<li>(3) <strong>32,716</strong> unique annotated cars. <strong>58,247</strong> unique negative examples.</li>
<li>(4) Intentional selection of hard negative examples.</li>
<li>(5) Established baseline for detection and counting tasks.</li>
<li>(6) Extra testing scenes for use after validation.</li>
</ul>
<p><strong>Reference:</strong></p>
<ul>
<li><em>Mundhenk T N, Konjevod G, Sakla W A, et al. A large contextual dataset for classification, detection and counting of cars with deep learning[C]//European Conference on Computer Vision. Springer, Cham, 2016: 785-800.</em></li>
</ul>
<h2 id="4-DOTA"><a href="#4-DOTA" class="headerlink" title="4. DOTA"></a>4. DOTA</h2><p>DOTA: A Large-scale Dataset for Object Detection in Aerial Images<br>DOTA Dataset Page: <a href="https://captain-whu.github.io/DOTA/index.html" target="_blank" rel="noopener">https://captain-whu.github.io/DOTA/index.html</a><br><a href="http://captain.whu.edu.cn/DOTAweb/" target="_blank" rel="noopener">http://captain.whu.edu.cn/DOTAweb/</a><br><a href="https://captain-whu.github.io/ODAI/" target="_blank" rel="noopener">https://captain-whu.github.io/ODAI/</a><br><a href="https://arxiv.org/abs/1711.10398" target="_blank" rel="noopener">https://arxiv.org/abs/1711.10398</a><br><img src="https://captain-whu.github.io/DOTA/instances-DOTA.jpg" alt=""><br>Dota is a large-scale dataset for object detection in aerial images. It can be used to develop and evaluate object detectors in aerial images. For the DOTA-v1.0, as described in the paper, it contains <strong>2806</strong> aerial images from different sensors and platforms. Each image is of the size in the range from about <strong>800 × 800</strong> to <strong>4000 × 4000</strong> pixels and contains objects exhibiting a wide variety of scales, orientations, and shapes. These DOTA images are then annotated by experts in aerial image interpretation using<strong> 15 common object categories</strong>. The fully annotated DOTA images contains <strong>188, 282</strong> instances, each of which is labeled by an arbitrary (8 d.o.f.) quadrilateral.<br><strong>Reference:</strong></p>
<ul>
<li><em>Xia G S, Bai X, Ding J, et al. DOTA: A Large-scale Dataset for Object Detection in Aerial Images[J]. arXiv preprint arXiv:1711.10398, 2017.</em></li>
<li><em>Gui-Song Xia, Xiang Bai, Jian Ding, Zhen Zhu, Serge Belongie, Jiebo Luo, Mihai Datcu, Marcello Pelillo, Liangpei Zhang; The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018, pp. 3974-3983</em></li>
</ul>
<h2 id="5-DIUx-xView-2018-Detection-Challenge"><a href="#5-DIUx-xView-2018-Detection-Challenge" class="headerlink" title="5. DIUx xView 2018 Detection Challenge"></a>5. DIUx xView 2018 Detection Challenge</h2><p><a href="http://xviewdataset.org/" target="_blank" rel="noopener">http://xviewdataset.org/</a><br><a href="http://challenge.xviewdataset.org" target="_blank" rel="noopener">http://challenge.xviewdataset.org</a><br><a href="https://arxiv.org/abs/1802.07856" target="_blank" rel="noopener">https://arxiv.org/abs/1802.07856</a><br><img src="http://xviewdataset.org/img/example6.jpg" alt=""><br><img src="http://challenge.xviewdataset.org/static/example_labeled.jpg" alt=""><br>xView is one of the largest publicly available sets of overhead imagery. It contains images from complex scenes around the world, annotated with <strong>more than one million bounding boxes</strong> representing a diverse range of <strong>60 object classes</strong>. Compared to other overhead imagery datasets, xView images are high-resolution, multi-spectral, and labeled with a greater variety of objects.</p>
<p>Given a high-resolution satellite image, the Challenge task is to predict a bounding box for each object in the image. The DIUx xView Challenge is focused on accelerating progress in four computer vision frontiers: Reduce minimum resolution for detection; Improve learning efficiency; Enable discovery of more object classes; Improve detection of fine-grained classes.</p>
<p>The DIUx xView Challenge follows in the footsteps of Challenges such as Common Objects in Context (COCO) and seeks to build off SpaceNet and Functional Map of the World (FMoW) to apply computer vision to the growing amount of available imagery from space so that we can understand the visual world in new ways and address a range of important applications.<br><strong>Reference:</strong></p>
<ul>
<li><em>Lam D, Kuzma R, McGee K, et al. xView: Objects in Context in Overhead Imagery[J]. arXiv preprint arXiv:1802.07856, 2018.</em></li>
</ul>
<h1 id="语义分割"><a href="#语义分割" class="headerlink" title="语义分割"></a>语义分割</h1><h2 id="1-Zurich-Summer-Dataset"><a href="#1-Zurich-Summer-Dataset" class="headerlink" title="1. Zurich Summer Dataset"></a>1. Zurich Summer Dataset</h2><p><a href="https://sites.google.com/site/michelevolpiresearch/data/zurich-dataset" target="_blank" rel="noopener">https://sites.google.com/site/michelevolpiresearch/data/zurich-dataset</a><br><img src="https://sites.google.com/site/michelevolpiresearch/_/rsrc/1438622321150/data/zurich-dataset/zh8_web.png?height=206&amp;width=320" alt=""><img src="https://sites.google.com/site/michelevolpiresearch/_/rsrc/1438622318886/data/zurich-dataset/zh8_gt_web.png?height=206&amp;width=320" alt=""></p>
<p>The “Zurich Summer v1.0” dataset is a collection of <strong>20 chips (crops)</strong>, taken from a QuickBird acquisition of the city of Zurich (Switzerland) in August 2002. QuickBird images are composed by 4 channels (NIR-R-G-B) and were pansharpened to the PAN resolution of about <strong>0.62 cm GSD</strong>. We manually annotated <strong>8</strong> different urban and periurban classes : <font color="red">Roads, Buildings, Trees, Grass, Bare Soil, Water, Railways and Swimming pools</font>. The cumulative number of class samples is highly unbalanced, to reflect real world situations. Note that annotations are not perfect, are not ultradense (not every pixel is annotated) and there might be some errors as well. We performed annotations by jointly selecting superpixels (SLIC) and drawing (freehand) over regions which we could confidently assign an object class.<br><strong>Reference:</strong></p>
<ul>
<li><em>Volpi, M. &amp; Ferrari, V.; Semantic segmentation of urban scenes by learning local class interactions, In IEEE CVPR 2015 Workshop “Looking from above: when Earth observation meets vision” (EARTHVISION), Boston, USA, 2015.</em></li>
<li><em>Volpi, M. &amp; Ferrari, V.; Structured prediction for urban scene semantic segmentation with geographic context, In Joint Urban Remote Sensing Event JURSE 2015, Lausanne, Switzerland, 2015.</em></li>
</ul>
<h2 id="2-ISPRS-Test-Project-on-Urban-Classification-and-3D-Building-Reconstruction–2D-Semantic-Labeling-Contest"><a href="#2-ISPRS-Test-Project-on-Urban-Classification-and-3D-Building-Reconstruction–2D-Semantic-Labeling-Contest" class="headerlink" title="2. ISPRS Test Project on Urban Classification and 3D Building Reconstruction–2D Semantic Labeling Contest"></a>2. ISPRS Test Project on Urban Classification and 3D Building Reconstruction–2D Semantic Labeling Contest</h2><p><a href="http://www2.isprs.org/commissions/comm3/wg4/semantic-labeling.html" target="_blank" rel="noopener">http://www2.isprs.org/commissions/comm3/wg4/semantic-labeling.html</a><br>To this end we provide two state-of-the-art airborne image datasets, consisting of very high resolution <strong>true ortho photo (TOP)</strong> tiles and corresponding <strong>digital surface models (DSMs)</strong> derived from dense image matching techniques. Both areas cover urban scenes. While <strong>Vaihingen</strong> is a relatively small village with many detached buildings and small multi story buildings, <strong>Potsdam</strong> shows a typical historic city with large building blocks, narrow streets and dense settlement structure.</p>
<p>Each dataset has been classified manually into <strong>six most common land cover classes</strong>. We provide the classification data (label images) for approximately half of the images, while the ground truth of the remaining scenes will remain unreleased and stays with the benchmark test organizers to be used for evaluation of submitted results. Participants shall use all data with ground truth for training or internal evaluation of their method.</p>
<p><strong>Six</strong> categories/classes have been defined:</p>
<ul>
<li><strong>Impervious surfaces</strong> (RGB: 255, 255, 255)</li>
<li><strong>Building</strong> (RGB: 0, 0, 255)</li>
<li><strong>Low vegetation</strong> (RGB: 0, 255, 255)</li>
<li><strong>Tree</strong> (RGB: 0, 255, 0)</li>
<li><strong>Car</strong> (RGB: 255, 255, 0)</li>
<li><strong>Clutter/background</strong> (RGB: 255, 0, 0)</li>
</ul>
<p><strong>The clutter/background class includes water bodies (present in two images with part of a river) and other objects that look very different from everything else (e.g., containers, tennis courts, swimming pools) and that are usually not of interest in semantic object classification in urban scenes, however note that participants must submit labels for all classes (including the clutter/background class). </strong>For instance, it is not possible to submit only classification results for the category building.</p>
<h3 id="Vaihingen"><a href="#Vaihingen" class="headerlink" title="Vaihingen"></a>Vaihingen</h3><p><img src="http://www2.isprs.org/tl_files/isprs/wg34/images/overview_tiles.jpg" alt=""><br>The data set contains <strong>33 patches (of different sizes)</strong>, each consisting of a true orthophoto (TOP) extracted from a larger TOP mosaic<br><strong>The ground sampling distance of both, the TOP and the DSM, is 9 cm</strong>. The DSM was generated via dense image matching with Trimble INPHO 5.3 software and Trimble INPHO OrthoVista was used to generate the TOP mosaic. In order to avoid areas without data (“holes”) in the TOP and the DSM, the patches were selected from the central part of the TOP mosaic and none at the boundaries. Remaining (very small) holes in the TOP and the DSM were interpolated.<br>The TOP are <strong>8 bit</strong> TIFF files with three bands; the three RGB bands of the TIFF files correspond to the near infrared, red and green bands delivered by the camera.<br>The DSM are TIFF files with one band; the grey levels (corresponding to the DSM heights) are encoded as <strong>32 bit</strong> float values.<br>The TOP and the DSM are defined on the same grid, so that it is not necessary to consider the geocoding information in the processing.</p>
<h3 id="Potsdam"><a href="#Potsdam" class="headerlink" title="Potsdam"></a>Potsdam</h3><p><img src="http://www2.isprs.org/tl_files/isprs/wg34/images/tile_overview.resized.png" alt=""><br>The data set contains <strong>38 patches (of the same size)</strong>, each consisting of a true orthophoto (TOP) extracted from a larger TOP mosaic<br>The ground sampling distance of both, the TOP and the DSM, is <strong>5 cm</strong>. The DSM was generated via dense image matching with Trimble INPHO 5.6 software and Trimble INPHO OrthoVista was used to generate the TOP mosaic. In order to avoid areas without data (“holes”) in the TOP and the DSM, the patches were selected from the central part of the TOP mosaic and none at the boundaries. Remaining (very small) holes in the TOP and the DSM were interpolated.<br>The TOP come as TIFF files in different channel composistions, where each channel has a spectral resolution of 8bit:</p>
<ul>
<li><strong>IRRG: 3 channels (IR-R-G)</strong></li>
<li><strong>RGB: 3 channels (R-G-B)</strong></li>
<li><strong>RGBIR: 4 channels (R-G-B-IR)</strong></li>
</ul>
<p>In this way participants can pick the data needed conveniently.<br>The DSM are TIFF files with one band; the grey levels (corresponding to the DSM heights) are encoded as <strong>32 bit</strong> float values. The TOP and the DSM are defined on the same grid (UTM WGS84). Each tile comes with an affine transformation file (tiff world file) in order to enable a re-composition of images to larger mosaics if desired.</p>
<p>In addition to the DSMs we provide so-called <strong>normalised DSMs</strong>, that is, after ground filtering the ground height is removed for each pixel, leading to an representation of heights above the terrain. This data was produced using some fully automatic filtering workflow, without manual quality control. Hence, we do not guarantee error free data here, this is just for researchers to help using height data, other than the absolute DSM. In the download folder you find the corresponding zip-file. If you unpack you find a readme.txt which you should read before using the data. The scripts based on lastools are also contained, so participants might want to tune them. We however do not provide support.</p>
<h2 id="3-2017-IEEE-GRSS-Data-Fusion-Contest"><a href="#3-2017-IEEE-GRSS-Data-Fusion-Contest" class="headerlink" title="3. 2017 IEEE GRSS Data Fusion Contest"></a>3. 2017 IEEE GRSS Data Fusion Contest</h2><p><a href="http://www.grss-ieee.org/2017-ieee-grss-data-fusion-contest/" target="_blank" rel="noopener">http://www.grss-ieee.org/2017-ieee-grss-data-fusion-contest/</a><br><a href="http://dase.ticinumaerospace.com/index.php" target="_blank" rel="noopener">http://dase.ticinumaerospace.com/index.php</a><br><img src="http://www.grss-ieee.org/wp-content/uploads/2016/12/banner.png" alt=""><br>The 2017 Data Fusion Contest will consist in a classification benchmark. The task to perform is classification of land use (more precisely, Local Climate Zones, LCZ, Stewart and Oke, 2012) in various urban environments. Several cities have been selected to test the ability of LCZ prediction at generalizing all over the world. Input data are multi-temporal, multi-source and multi-modal (image and semantic layers).</p>
<p>Local climate zones are a generic, climate-based typology of urban and natural landscapes, which delivers information on basic physical properties of an area that can be used by land use planners or climate modelers. LCZ are used as first order discretization of urban areas by the World Urban Database and Access Portal Tools initiative, which aims to collect, store and disseminate data on the form and function of cities around the world.</p>
<p><img src="https://i.imgur.com/70AzK8b.png" alt=""><br>The LCZ classes in this study correspond to those of [Stewart &amp; Oke, 2012]:</p>
<p>10 urban LCZs corresponding to various built types:</p>
<ul>
<li>Compact high-rise (class code in the ground truth: 1);</li>
<li>Compact midrise (class code in the ground truth: 2);</li>
<li>Compact low-rise (class code in the ground truth: 3);</li>
<li>Open high-rise (class code in the ground truth: 4);</li>
<li>Open midrise (class code in the ground truth: 5);</li>
<li>Open low-rise (class code in the ground truth: 6);</li>
<li>Lightweight low-rise (class code in the ground truth: 7);</li>
<li>Large low-rise (class code in the ground truth: 8);</li>
<li>Sparsely built (class code in the ground truth: 9);</li>
<li>Heavy industry (class code in the ground truth: 10).</li>
</ul>
<p>7 rural LCZs corresponding to various land cover types:</p>
<ul>
<li>Dense trees (class code in the ground truth: 11);</li>
<li>Scattered trees (class code in the ground truth: 12);</li>
<li>Bush and scrub (class code in the ground truth: 13);</li>
<li>Low plants (class code in the ground truth: 14);</li>
<li>Bare rock or paved (class code in the ground truth: 15);</li>
<li>Bare soil or sand (class code in the ground truth: 16);</li>
<li>Water (class code in the ground truth: 17).</li>
</ul>
<p>The contest aims to promote innovation in <strong>classification algorithms</strong>, as well as to provide objective and fair comparisons among methods. Ranking is based on quantitative accuracy parameters computed with respect to undisclosed test samples from cities unseen during training. Participants will be given a limited time to submit their classification maps after the competition is started. The contest will consist of two steps:</p>
<ul>
<li><p><strong>Step 1 – training: Participants are provided with five training cities (Berlin, Rome, Paris, Sao Paulo, Hong Kong), including ground truth to train their algorithms.</strong></p>
</li>
<li><p><strong>Step 2 – testing on new cities: Participants will receive the data of the test cities and will submit their classification maps by three weeks from the release of this second part of the data set. In parallel, they will submit a short description of the approach used. After evaluation of the results, 4 winners will be announced.</strong></p>
</li>
</ul>
<p><strong>The Data:</strong></p>
<ul>
<li><strong>Landsat data</strong>, in the form of images with <strong>8 multispectral bands</strong> (i.e. visible, short and long infrared wavelengths) resampled at <strong>100m resolution</strong> (courtesy of the U.S. Geological Survey);</li>
<li><strong>Sentinel2</strong> images, with <strong>9 multispectral bands</strong> (i.e. visible, vegetation red edges and short infrared wavelengths) resampled at <strong>100m resolution</strong> (Contains modified Copernicus Data 2016); participants are encouraged to use the full resolution data, for which a direct link is provided in the data package.</li>
<li><strong>Ancillary data</strong>: <strong>Open Street Map (OSM)</strong> layers with land use information: building, natural, roads and land-use areas. We also provide <strong>rasterized versions of OSM layers at 20m resolution</strong> for building and land-use areas, superimposable with the satellite images.</li>
<li>Moreover, for the training cities only, we also provide <strong>ground-truth of the various LCZ classes on several areas of the city</strong> (defined as polygons using the class codes above). They are provided as raster layers at 100m resolution, superimposable to the satellite images. The ground-truth for the test set will remain undisclosed and will be used for evaluation of the results.</li>
</ul>
<h2 id="4-2018-IEEE-GRSS-Data-Fusion-Contest–Advanced-multi-sensor-optical-remote-sensing-for-urban-land-use-and-land-cover-classification"><a href="#4-2018-IEEE-GRSS-Data-Fusion-Contest–Advanced-multi-sensor-optical-remote-sensing-for-urban-land-use-and-land-cover-classification" class="headerlink" title="4. 2018 IEEE GRSS Data Fusion Contest–Advanced multi-sensor optical remote sensing for urban land use and land cover classification"></a>4. 2018 IEEE GRSS Data Fusion Contest–Advanced multi-sensor optical remote sensing for urban land use and land cover classification</h2><p><a href="http://www.grss-ieee.org/community/technical-committees/data-fusion/2018-ieee-grss-data-fusion-contest/" target="_blank" rel="noopener">http://www.grss-ieee.org/community/technical-committees/data-fusion/2018-ieee-grss-data-fusion-contest/</a><br><a href="http://dase.ticinumaerospace.com" target="_blank" rel="noopener">http://dase.ticinumaerospace.com</a><br><img src="http://www.grss-ieee.org/wp-content/uploads/2018/01/dfc2018_header_v4.png" alt=""><br>The 2018 IEEE GRSS Data Fusion Contest, organized by the Image Analysis and Data Fusion Technical Committee, aims to promote progress on fusion and analysis methodologies for multi-source remote sensing data.</p>
<p>The 2018 Data Fusion Contest consists of a classification benchmark. The task to be performed is urban land use and land cover classification. </p>
<p>To test their synergy as well as individual potential for urban land use and land cover classification, classification results can be submitted to three parallel and independent competitions:</p>
<ul>
<li><strong>Data Fusion Classification Challenge</strong> (use of at least two data sets)</li>
<li><strong>Multispectral LiDAR Classification Challenge</strong></li>
<li><strong>Hyperspectral Classification Challenge</strong></li>
</ul>
<p><strong>The Data:</strong><br>We provide (Acquired by the National Center for Airborne Laser Mapping, NCALM): The data were acquired by NCALM on February 16, 2017 between 16:31 and 18:18 GMT. Sensors used in this campaign include an Optech Titam MW (14SEN/CON340) with integrated camera (a LIDAR sensor operating at three different laser wavelengths), a DiMAC ULTRALIGHT+ (a very high resolution color imager) with a 70 mm focal length, and an ITRES CASI 1500 (a hyperspectral imager).</p>
<ul>
<li><strong>Multispectral-LiDAR point cloud data</strong> at 1550 nm, 1064 nm, and 532 nm; <strong>Intensity rasters</strong> from first return per channel and <strong>DSMs</strong> at a <strong>50-cm GSD</strong>.</li>
<li><strong>Hyperspectral data</strong> covering a 380-1050 nm spectral range with <strong>48 bands</strong> at a <strong>1-m GSD</strong>.</li>
<li><strong>Very high resolution RGB imagery</strong> at a <strong>5-cm GSD</strong>. The image is organized into several separate tiles.</li>
</ul>
<p>The data were acquired by NCALM on February 16, 2017 between 16:31 and 18:18 GMT. Sensors used in this campaign include an Optech Titam MW (14SEN/CON340) with integrated camera (a LIDAR sensor operating at three different laser wavelengths), a DiMAC ULTRALIGHT+ (a very high-resolution color imager) with a 70mm focal length, and an ITRES CASI 1500 (a hyperspectral imager). The sensors were aboard a Piper PA-31- 350 Navajo Chieftain aircraft.</p>
<p>Moreover, for the training region only, we also provide <strong>ground-truth corresponding to 20 urban land use and land cover classes</strong>. They are provided as raster at a 0.5-m GSD, superimposable to airborne images.</p>
<p>The ground truth for the test set remains undisclosed and will be used for evaluation of the results.</p>
<p>Urban Land Use and Land Cover Classes:</p>
<ul>
<li>0 – Unclassified</li>
<li>1 – Healthy grass</li>
<li>2 – Stressed grass</li>
<li>3 – Artificial turf</li>
<li>4 – Evergreen trees</li>
<li>5 – Deciduous trees</li>
<li>6 – Bare earth</li>
<li>7 – Water</li>
<li>8 – Residential buildings</li>
<li>9 – Non-residential buildings</li>
<li>10 – Roads</li>
<li>11 – Sidewalks</li>
<li>12 – Crosswalks</li>
<li>13 – Major thoroughfares</li>
<li>14 – Highways</li>
<li>15 – Railways</li>
<li>16 – Paved parking lots</li>
<li>17 – Unpaved parking lots</li>
<li>18 – Cars</li>
<li>19 – Trains</li>
<li>20 – Stadium seats</li>
</ul>
<h2 id="5-EvLab-SS-Dataset"><a href="#5-EvLab-SS-Dataset" class="headerlink" title="5. EvLab-SS Dataset"></a>5. EvLab-SS Dataset</h2><p><a href="http://earthvisionlab.whu.edu.cn/zm/SemanticSegmentation/index.html" target="_blank" rel="noopener">http://earthvisionlab.whu.edu.cn/zm/SemanticSegmentation/index.html</a><br>The EvLab-SS benchmark is designed for the evaluation of the semantic segmentation algorithms on real engineered scenes, which aims to find a good deep learning architecture for the high resolution pixel-wise classification task in remote sensing area.<br>The dataset is originally obtained from the Chinese Geographic Condition Survey and Mapping Project, and each image is fully annotated by the Geographic Conditions Survey (NO.GDPJ 01—2013) standards. The average resolution of the dataset is approximately <strong>4500 × 4500</strong> pixels. The EvLab-SS dataset contains <strong>11 major classes</strong>, namely, <font color="red">background, farmland, garden, woodland, grassland, building, road, structures, digging pile, desert and waters</font>, and currently includes <strong>60 frames of images</strong> captured by <strong>different platforms and sensors</strong>. The dataset comprises <strong>35 satellite images</strong>, <strong>19 frames</strong> of which are captured by the <strong>World-View-2 satellite (re-sample GSD 0.2 m)</strong>, <strong>5 frames</strong> are captured by the <strong>GeoEye satellite (re-sample GSD 0.5 m)</strong>, <strong>5 frames </strong>are captured by the <strong>QuickBird satellite (re-sample GSD 2 m)</strong>, <strong>6 frames</strong> are captured by the <strong>GF-2 satellite (re-sample GSD 1 m)</strong>. The dataset also has <strong>25 aerial images</strong>, <strong>10 images</strong> of which with spatial resolution of <strong>0.25 m</strong> and <strong>15 images</strong> have a spatial resolution of <strong>0.1 m</strong>.<br><strong>Reference:</strong></p>
<ul>
<li><em>Zhang M, Hu X, Zhao L, et al. Learning dual multi-scale manifold ranking for semantic segmentation of high-resolution images[J]. Remote Sensing, 2017, 9(5): 500.</em></li>
</ul>
<h2 id="6-DeepGlobe-Land-Cover-Classification-Challenge"><a href="#6-DeepGlobe-Land-Cover-Classification-Challenge" class="headerlink" title="6. DeepGlobe Land Cover Classification Challenge"></a>6. DeepGlobe Land Cover Classification Challenge</h2><p><img src="http://deepglobe.org/uploads/1/1/7/0/117046351/published/screen-shot-2018-02-16-at-4-08-32-pm.png?1518826243" alt=""><br><a href="http://deepglobe.org/index.html" target="_blank" rel="noopener">http://deepglobe.org/index.html</a><br><a href="https://competitions.codalab.org/competitions/18468" target="_blank" rel="noopener">https://competitions.codalab.org/competitions/18468</a><br>Automatic categorization and segmentation of land cover is of great importance for sustainable development, autonomous agriculture, and urban planning. We would like to introduce the challenge of automatic classification of land cover types. This problem is defined as a multi-class segmentation task to detect areas of <font color="red">urban, agriculture, rangeland, forest, water, barren, and unknown</font>. The evaluation will be based on the accuracy of the class labels.<br><strong>Reference:</strong></p>
<ul>
<li><em>Demir I, Koperski K, Lindenbaum D, et al. DeepGlobe 2018: A Challenge to Parse the Earth through Satellite Images[J]. arXiv preprint arXiv:1805.06561, 2018.</em></li>
</ul>
<h2 id="7-Gaofen-Image-Dataset-GID"><a href="#7-Gaofen-Image-Dataset-GID" class="headerlink" title="7. Gaofen Image Dataset (GID)"></a>7. Gaofen Image Dataset (GID)</h2><p><a href="http://captain.whu.edu.cn/GID/" target="_blank" rel="noopener">http://captain.whu.edu.cn/GID/</a><br>Gaofen Image Dataset (GID) a large-scale dataset for land use and land cover (LULC) classification. It contains 150 high-quality Gaofen-2 (GF-2) images acquired from more than 60 different cities in China. And these images cover the geographic areas that exceed 50,000 km2. Images in GID have high intra-class diversity coupled with low inter-class separability. Therefore, GID can provide the research community with a high-quality data resource to advance the state-of-the-art in LULC classification.<br><strong>Reference:</strong></p>
<ul>
<li><em>Tong X Y, Xia G S, Lu Q, et al. Learning Transferable Deep Models for Land-Use Classification with High-Resolution Remote Sensing Images[J]. arXiv preprint arXiv:1807.05713, 2018.</em></li>
</ul>
<h2 id="8-Airbus-Ship-Detection-Challenge"><a href="#8-Airbus-Ship-Detection-Challenge" class="headerlink" title="8. Airbus Ship Detection Challenge"></a>8. Airbus Ship Detection Challenge</h2><p><a href="https://www.kaggle.com/c/airbus-ship-detection" target="_blank" rel="noopener">https://www.kaggle.com/c/airbus-ship-detection</a></p>
<h1 id="建筑物检测"><a href="#建筑物检测" class="headerlink" title="建筑物检测"></a>建筑物检测</h1><h2 id="1-Massachusetts-Buildings-Dataset"><a href="#1-Massachusetts-Buildings-Dataset" class="headerlink" title="1. Massachusetts Buildings Dataset"></a>1. Massachusetts Buildings Dataset</h2><p><a href="https://www.cs.toronto.edu/~vmnih/data/" target="_blank" rel="noopener">https://www.cs.toronto.edu/~vmnih/data/</a></p>
<p>The size of all images in these datasets is <strong>1500×1500</strong>, and the resolution is <strong>1m2/pixel</strong>. </p>
<p>The buildingdataset consists of <strong>137</strong> sets of aerial images and corresponding single-channel label images for training part, <strong>10</strong> for testing part, and <strong>4</strong> for validation part. </p>
<p><strong>Reference:</strong></p>
<ul>
<li><em>Mnih V. Machine learning for aerial image labeling[D]. University of Toronto (Canada), 2013.</em></li>
</ul>
<h2 id="2-SpaceNet-Buildings-Dataset"><a href="#2-SpaceNet-Buildings-Dataset" class="headerlink" title="2. SpaceNet Buildings Dataset"></a>2. SpaceNet Buildings Dataset</h2><p><a href="https://spacenetchallenge.github.io/" target="_blank" rel="noopener">https://spacenetchallenge.github.io/</a><br>The Data - Over 685,000 footprints across the Five SpaceNet Areas of Interest.</p>
<table>
<thead>
<tr>
<th style="text-align:center">AOI</th>
<th style="text-align:center">Area of Raster (Sq. Km)</th>
<th style="text-align:center">Building Labels (Polygons)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">AOI_1_Rio</td>
<td style="text-align:center">2,544</td>
<td style="text-align:center">382,534</td>
</tr>
<tr>
<td style="text-align:center">AOI_2_Vegas</td>
<td style="text-align:center">216</td>
<td style="text-align:center">151,367</td>
</tr>
<tr>
<td style="text-align:center">AOI_3_Paris</td>
<td style="text-align:center">1,030</td>
<td style="text-align:center">23,816</td>
</tr>
<tr>
<td style="text-align:center">AOI_4_Shanghai</td>
<td style="text-align:center">1,000</td>
<td style="text-align:center">92,015</td>
</tr>
<tr>
<td style="text-align:center">AOI_5_Khartoum</td>
<td style="text-align:center">765</td>
<td style="text-align:center">35,503</td>
</tr>
</tbody>
</table>
<h3 id="SpaceNet-Buildings-Dataset-Round-1"><a href="#SpaceNet-Buildings-Dataset-Round-1" class="headerlink" title="SpaceNet Buildings Dataset Round 1"></a>SpaceNet Buildings Dataset Round 1</h3><p><strong>Catalog</strong><br>The data is hosted on AWS in a requester pays bucket.<br><figure class="highlight armasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="symbol">aws</span> <span class="built_in">s3</span> ls <span class="built_in">s3</span>://spacenet-dataset/SpaceNet_Buildings_Dataset_Round1/</span><br></pre></td></tr></table></figure></p>
<p><strong>Training Data</strong><br>AOI 1 - Rio - Building Extraction Training<br>To download processed 200mx200m tiles of AOI 1 (23 GB) with associated building footprints for training do the following:<br><figure class="highlight armasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="symbol">aws</span> <span class="meta">cp</span> <span class="built_in">s3</span>://spacenet-dataset/spacenet_TrainData/<span class="number">3</span>band.tar.gz .</span><br><span class="line"><span class="symbol">aws</span> <span class="meta">cp</span> <span class="built_in">s3</span>://spacenet-dataset/spacenet_TrainData/<span class="number">8</span>band.tar.gz .</span><br></pre></td></tr></table></figure></p>
<p><strong>Test Data</strong><br>AOI 1 - Rio - Building Extraction Testing<br>To download processed 200mx200m tiles of AOI 1 (7.9 GB) for testing do:<br><figure class="highlight armasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="symbol">aws</span> <span class="meta">cp</span> <span class="built_in">s3</span>://spacenet-dataset/spacenet_TestData/<span class="number">3</span>band.tar.gz .</span><br><span class="line"><span class="symbol">aws</span> <span class="meta">cp</span> <span class="built_in">s3</span>://spacenet-dataset/spacenet_TestData/<span class="number">8</span>band.tar.gz .</span><br></pre></td></tr></table></figure></p>
<h3 id="SpaceNet-Buildings-Dataset-Round-2"><a href="#SpaceNet-Buildings-Dataset-Round-2" class="headerlink" title="SpaceNet Buildings Dataset Round 2"></a>SpaceNet Buildings Dataset Round 2</h3><p><strong>Catalog</strong><br>The data is hosted on AWS in a requester pays bucket.<br><figure class="highlight armasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="symbol">aws</span> <span class="built_in">s3</span> ls <span class="built_in">s3</span>://spacenet-dataset/SpaceNet_Buildings_Dataset_Round2/</span><br></pre></td></tr></table></figure></p>
<p><strong>Training Data (57.3 GB)</strong><br><figure class="highlight armasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="symbol">aws</span> <span class="built_in">s3</span> <span class="meta">cp</span> <span class="built_in">s3</span>://spacenet-dataset/SpaceNet_Buildings_Dataset_Round2/spacenetV2_Train/ . --recursive</span><br></pre></td></tr></table></figure></p>
<p>AOI 2 - Vegas - Building Extraction Training<br>To download processed 200mx200m tiles of AOI 2 (23 GB) with associated building footprints for training do the following:<br><figure class="highlight armasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="symbol">aws</span> <span class="built_in">s3</span> <span class="meta">cp</span> <span class="built_in">s3</span>://spacenet-dataset/SpaceNet_Buildings_Dataset_Round2/spacenetV2_Train/AOI_2_Vegas_Train.tar.gz .</span><br></pre></td></tr></table></figure></p>
<p>AOI 3 - Paris - Building Extraction Training<br>To download processed 200mx200m tiles of AOI 3 (5.3 GB) with associated building footprints do the following:<br><figure class="highlight armasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="symbol">aws</span> <span class="built_in">s3</span> <span class="meta">cp</span> <span class="built_in">s3</span>://spacenet-dataset/SpaceNet_Buildings_Dataset_Round2/spacenetV2_Train/AOI_3_Paris_Train.tar.gz .</span><br></pre></td></tr></table></figure></p>
<p>AOI 4 - Shanghai - Building Extraction Training<br>To download processed 200mx200m tiles of AOI 4 (23.4 GB) with associated building footprints do the following:<br><figure class="highlight armasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="symbol">aws</span> <span class="built_in">s3</span> <span class="meta">cp</span> <span class="built_in">s3</span>://spacenet-dataset/SpaceNet_Buildings_Dataset_Round2/spacenetV2_Train/AOI_4_Shanghai_Train.tar.gz .</span><br></pre></td></tr></table></figure></p>
<p>AOI 5 - Khartoum - Building Extraction Training<br>To download processed 200mx200m tiles of AOI 2 (4.7 GB) with associated building footprints do the following:<br><figure class="highlight armasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="symbol">aws</span> <span class="built_in">s3</span> <span class="meta">cp</span> <span class="built_in">s3</span>://spacenet-dataset/SpaceNet_Buildings_Dataset_Round2/spacenetV2_Train/AOI_5_Khartoum_Train.tar.gz .</span><br></pre></td></tr></table></figure></p>
<p><strong>Test Data (19 GB)</strong><br><figure class="highlight armasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="symbol">aws</span> <span class="built_in">s3</span> <span class="meta">cp</span> <span class="built_in">s3</span>://spacenet-dataset/SpaceNet_Buildings_Dataset_Round2/spacenetV2_Test/ . --recursive</span><br></pre></td></tr></table></figure></p>
<p>AOI 2 - Vegas - Building Extraction Testing<br>To download processed 200mx200m tiles of AOI 2 (7.9 GB) for testing do:<br><figure class="highlight armasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="symbol">aws</span> <span class="built_in">s3</span> <span class="meta">cp</span> <span class="built_in">s3</span>://spacenet-dataset/SpaceNet_Buildings_Dataset_Round2/spacenetV2_Test/AOI_2_Vegas_Test_public.tar.gz .</span><br></pre></td></tr></table></figure></p>
<p>AOI 3 - Paris - Building Extraction Testing<br>To download processed 200mx200m tiles of AOI 3 (1.8 GB) for testing do:<br><figure class="highlight armasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="symbol">aws</span> <span class="built_in">s3</span> <span class="meta">cp</span> <span class="built_in">s3</span>://spacenet-dataset/SpaceNet_Buildings_Dataset_Round2/spacenetV2_Test/AOI_3_Paris_Test_public.tar.gz .</span><br></pre></td></tr></table></figure></p>
<p>AOI 4 - Shanghai - Building Extraction Testing<br>To download processed 200mx200m tiles of AOI 4 (7.7 GB) for testing do:<br><figure class="highlight armasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="symbol">aws</span> <span class="built_in">s3</span> <span class="meta">cp</span> <span class="built_in">s3</span>://spacenet-dataset/SpaceNet_Buildings_Dataset_Round2/spacenetV2_Test/AOI_4_Shanghai_Test_public.tar.gz .</span><br></pre></td></tr></table></figure></p>
<p>AOI 5 - Khartoum - Building Extraction Testing<br>To download processed 200mx200m tiles of AOI 2 (1.6 GB) for testing do:<br><figure class="highlight armasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="symbol">aws</span> <span class="built_in">s3</span> <span class="meta">cp</span> <span class="built_in">s3</span>://spacenet-dataset/SpaceNet_Buildings_Dataset_Round2/spacenetV2_Test/AOI_5_Khartoum_Test_public.tar.gz .</span><br></pre></td></tr></table></figure></p>
<h2 id="3-Inria-Aerial-Image-Labeling-Dataset"><a href="#3-Inria-Aerial-Image-Labeling-Dataset" class="headerlink" title="3. Inria Aerial Image Labeling Dataset"></a>3. Inria Aerial Image Labeling Dataset</h2><p><a href="https://project.inria.fr/aerialimagelabeling/" target="_blank" rel="noopener">https://project.inria.fr/aerialimagelabeling/</a><br>The Inria Aerial Image Labeling addresses a core topic in remote sensing: the automatic pixelwise labeling of aerial imagery (link to paper).</p>
<p>Dataset features:</p>
<p>Coverage of 810 km² (405 km² for training and 405 km² for testing)<br>Aerial orthorectified color imagery with a <strong>spatial resolution of 0.3 m</strong><br>Ground truth data for <strong>two semantic classes</strong>: building and not building (publicly disclosed only for the training subset)<br>The images cover dissimilar urban settlements, ranging from densely populated areas (e.g., San Francisco’s financial district) to alpine towns (e.g,. Lienz in Austrian Tyrol).</p>
<p>Instead of splitting adjacent portions of the same images into the training and test subsets, different cities are included in each of the subsets. For example, images over Chicago are included in the training set (and not on the test set) and images over San Francisco are included on the test set (and not on the training set). The ultimate goal of this dataset is to assess the generalization power of the techniques: while Chicago imagery may be used for training, the system should label aerial images over other regions, with varying illumination conditions, urban landscape and time of the year.</p>
<p><img src="https://project.inria.fr/aerialimagelabeling/files/2011/12/chi1-300x300.jpg" alt=""><img src="https://project.inria.fr/aerialimagelabeling/files/2011/12/chi2-300x300.jpg" alt=""></p>
<p><strong>Reference:</strong></p>
<ul>
<li><em>Maggiori E, Tarabalka Y, Charpiat G, et al. Can semantic labeling methods generalize to any city? The INRIA aerial image labeling benchmark[C]//IEEE International Symposium on Geoscience and Remote Sensing (IGARSS). 2017.</em></li>
</ul>
<h2 id="4-The-USSOCOM-Urban-3D-Challenge"><a href="#4-The-USSOCOM-Urban-3D-Challenge" class="headerlink" title="4. The USSOCOM Urban 3D Challenge"></a>4. The USSOCOM Urban 3D Challenge</h2><p><a href="https://www.topcoder.com/urban3d" target="_blank" rel="noopener">https://www.topcoder.com/urban3d</a><br><a href="https://spacenetchallenge.github.io/datasets/Urban_3D_Challenge_summary.html" target="_blank" rel="noopener">https://spacenetchallenge.github.io/datasets/Urban_3D_Challenge_summary.html</a><br><a href="https://github.com/topcoderinc/Urban3d" target="_blank" rel="noopener">https://github.com/topcoderinc/Urban3d</a><br>This challenge published a large-scale dataset containing <strong>2D orthrorectified RGB</strong> and <strong>3D Digital Surface Models</strong> and <strong>Digital Terrain Models</strong> generated from commercial satellite imagery covering over 360 km of terrain and containing roughly <strong>157,000</strong> annotated building footprints. All imagery products are provided at <strong>50 cm ground sample distance (GSD)</strong>. This unique 2D/3D large scale dataset provides researchers an opportunity to utilize machine learning techniques to further improve state of the art performance.</p>
<p>Reliable labeling of buildings based on satellite imagery is one of the first and most challenging steps in producing accurate 3D models and maps. While automated algorithms continue to improve, significant manual effort is still necessary to ensure geospatial accuracy and acceptable quality. Improved automation is required to enable more rapid response to major world events such as humanitarian and disaster response. 3D height data can help improve automated building labeling performance, and capabilities for providing this data on a global scale are now emerging. In this challenge, we ask solvers to use satellite imagery and newly available 3D height data products to improve upon the state of the art for automated building detection and labeling.</p>
<p>USSOCOM is seeking an algorithm that provides reliable, automatic labeling of buildings based solely on orthorectified color satellite imagery and 3D height data.</p>
<p><strong>Reference:</strong></p>
<ul>
<li><em>H. Goldberg, M. Brown, and S. Wang, A Benchmark for Building Footprint Classification Using Orthorectified RGB Imagery and Digital Surface Models from Commercial Satellites, 46th Annual IEEE Applied Imagery Pattern Recognition Workshop, Washington, D.C, 2017.</em></li>
<li><em>H. Goldberg, S. Wang, M. Brown, and G. Christie. Urban 3D Challenge: Building Footprint Detection Using Orthorectified Imagery and Digital Surface Models from Commercial Satellites. In Proceedings SPIE Defense and Commercial Sensing: Geospatial Informatics and Motion Imagery Analytics VIII, Orlando, Florida, USA, 2018.</em></li>
</ul>
<h2 id="5-DeepGlobe-Building-Extraction-Challenge"><a href="#5-DeepGlobe-Building-Extraction-Challenge" class="headerlink" title="5. DeepGlobe Building Extraction Challenge"></a>5. DeepGlobe Building Extraction Challenge</h2><p><img src="http://deepglobe.org/uploads/1/1/7/0/117046351/published/picture1.png?1518826212" alt=""><br><a href="http://deepglobe.org/index.html" target="_blank" rel="noopener">http://deepglobe.org/index.html</a><br><a href="https://competitions.codalab.org/competitions/18544" target="_blank" rel="noopener">https://competitions.codalab.org/competitions/18544</a><br>Modeling population dynamics is of great importance for disaster response and recovery, and detection of buildings and urban areas are key to achieve so. We would like to pose the challenge of automatically detecting buildings from satellite images. This problem is formulated as a binary segmentation problem to localize all building polygons in each area. The evaluation will be based on the overlap of detected polygons with the ground truth.<br><strong>Reference:</strong></p>
<ul>
<li><em>Demir I, Koperski K, Lindenbaum D, et al. DeepGlobe 2018: A Challenge to Parse the Earth through Satellite Images[J]. arXiv preprint arXiv:1805.06561, 2018.</em></li>
</ul>
<h2 id="6-CrowdAI-Mapping-Challenge"><a href="#6-CrowdAI-Mapping-Challenge" class="headerlink" title="6. CrowdAI Mapping Challenge"></a>6. CrowdAI Mapping Challenge</h2><p><a href="https://www.crowdai.org/challenges/mapping-challenge" target="_blank" rel="noopener">https://www.crowdai.org/challenges/mapping-challenge</a><br><img src="https://crowdai-shared.s3.eu-central-1.amazonaws.com/markdown_editor/mapping_challenge_1.jpg" alt=""><br>In this challenge you will be provided with a dataset of individual tiles of satellite imagery as RGB images, and their corresponding annotations of where an image is there a building. The goal is to train a model which given a new tile can annotate all buildings.<br>Datasets</p>
<ul>
<li>train.tar.gz : This is the Training Set of <strong>280741</strong> tiles (as <strong>300x300</strong> pixel RGB images) of satellite imagery, along with their corresponding annotations in MS-COCO format</li>
<li>val.tar.gz: This is the suggested Validation Set of <strong>60317</strong> tiles (as <strong>300x300</strong> pixel RGB images) of satellite imagery, along with their corresponding annotations in MS-COCO format</li>
<li>test_images.tar.gz : This is the Test Set for Round-1, where you are provided with <strong>60697</strong> files (as <strong>300x300</strong> pixel RGB images) and your are required to submit annotations for all these files.</li>
</ul>
<h2 id="7-WHU-Building-Dataset"><a href="#7-WHU-Building-Dataset" class="headerlink" title="7. WHU Building Dataset"></a>7. WHU Building Dataset</h2><p><a href="http://study.rsgis.whu.edu.cn/pages/download/" target="_blank" rel="noopener">http://study.rsgis.whu.edu.cn/pages/download/</a><br><a href="http://study.rsgis.whu.edu.cn/pages/download/" target="_blank" rel="noopener">http://study.rsgis.whu.edu.cn/pages/download/</a><br>This dataset consists of an aerial dataset and a satellite dataset.</p>
<p>1、    Aerial imagery dataset<br>The original aerial data comes from the New Zealand Land Information Services website. We manually edited Christchurch’s building vector data, with about 22,000 independent buildings. The original ground resolution of the images is 0.075m.</p>
<p>we provide manually edited shapefile corresponds to the whole area.</p>
<p>We also down-sampled the most parts of aerial images (including 18,7000 buildings) to 0.3m ground resolution, and cropped them into 8,189 tiles with 512×512 pixels. The shapefile is also rasterized. The ready-to-use samples are divided into three parts: a training set (130,500 buildings), a validation set (14,500 buildings) and a test set (42,000 buildings).</p>
<p>Training area:4736 tiles, Evaluation area: 1036 tiles, Test area: 2416 tiles<br><img src="http://study.rsgis.whu.edu.cn/pages/download/instruction.files/image001.jpg" alt=""></p>
<p>2、 Satellite dataset I (global cities)<br>One of them is collected from cities over the world and from various remote sensing resources including QuickBird, Worldview series, IKONOS, ZY-3, etc. We manually delineated all the buildings. It contains 204 images (512 × 512 tiles with resolutions varying from 0.3 m to 2.5 m). Besides the differences in satellite sensors, the variations in atmospheric conditions, panchromatic and multispectral fusion algorithms, atmospheric and radiometric corrections and season made the samples suitable yet challenging for testing robustness of building extraction algorithms.<br><img src="http://study.rsgis.whu.edu.cn/pages/download/instruction.files/image002.jpg" alt=""></p>
<p>3、 Satellite dataset Ⅱ (East Asia)<br>The other satellite building sub-dataset consists of 6 neighboring satellite images covering 550 km2 on East Asia with 2.7 m ground resolution. This test area is mainly designed to evaluate and to develop the generalization ability of a deep learning method on different data sources but with similar building styles in the same geographical area. The vector building map is also fully manually delineated in ArcGIS software and contains 29085 buildings. The whole image is seamlessly cropped into 17388 512×512 tiles for convenient training and testing with the same processing as in our aerial dataset. Among them 21556 buildings (13662 tiles) are separated for training and the rest 7529 buildings (3726 tiles) are used for testing.</p>
<p>Training area: 13662 tiles<br>Test area: 3726 tiles<br><img src="http://study.rsgis.whu.edu.cn/pages/download/instruction.files/image003.png" alt=""></p>
<p>4、 Building change detection dataset<br>Our dataset covers an area where a 6.3-magnitude earthquake has occurred in February 2011 and rebuilt in the following years. This dataset consists of aerial images obtained in April 2012 that contains 12796 buildings in 20.5 km2 (16077 buildings in the same area in 2016 dataset). By manually selecting 30 GCPs on ground surface, the sub-dataset was geo-rectified to the aerial dataset with 1.6-pixel accuracy. This sub-dataset and the corresponding images from the original dataset are now openly provided along with building vector and raster maps.<br><img src="http://study.rsgis.whu.edu.cn/pages/download/instruction.files/image004.jpg" alt=""></p>
<h2 id="8-AIRS-Aerial-Imagery-for-Roof-Segmentation"><a href="#8-AIRS-Aerial-Imagery-for-Roof-Segmentation" class="headerlink" title="8. AIRS (Aerial Imagery for Roof Segmentation)"></a>8. AIRS (Aerial Imagery for Roof Segmentation)</h2><p><a href="https://www.airs-dataset.com/" target="_blank" rel="noopener">https://www.airs-dataset.com/</a><br>AIRS (Aerial Imagery for Roof Segmentation) is a public dataset that aims at benchmarking the algorithms of roof segmentation from very-high-resolution aerial imagery. The main features of AIRS can be summarized as:</p>
<ul>
<li>457km2 coverage of orthorectified aerial images with over 220,000 buildings</li>
<li>Very high spatial resolution of imagery (0.075m)</li>
<li>Refined ground truths that strictly align with roof outlines</li>
</ul>
<p>AIRS dataset covers almost the full area of Christchurch, the largest city in the South Island of New Zealand. The photography was taken during the flying seasons of 2015 and 2016, and the supplied images are ortho-rectified DOMs with RGB channels and 7.5cm resolution in projection of New Zealand Transverse Mercator. There are 226,342 labeled buildings within the whole area for experiment. To eliminate the impact of relief displacement, the ground truths for buildings are carefully refined to align with their roofs. Therefore, the segmentation task posed for AIRS contains two semantic classes: roof and non-roof pixels.<br><strong>Reference:</strong></p>
<ul>
<li><em>Chen Q, Wang L, Wu Y, et al. Aerial Imagery for Roof Segmentation: A Large-Scale Dataset towards Automatic Mapping of Buildings[J]. arXiv preprint arXiv:1807.09532, 2018.</em></li>
</ul>
<h2 id="9-2018-Open-AI-Tanzania-Building-Footprint-Segmentation-Challenge"><a href="#9-2018-Open-AI-Tanzania-Building-Footprint-Segmentation-Challenge" class="headerlink" title="9. 2018 Open AI Tanzania Building Footprint Segmentation Challenge"></a>9. 2018 Open AI Tanzania Building Footprint Segmentation Challenge</h2><p><a href="https://competitions.codalab.org/competitions/20100" target="_blank" rel="noopener">https://competitions.codalab.org/competitions/20100</a><br><img src="https://blog.werobotics.org/wp-content/uploads/2018/08/zmi-geonode.png" alt=""><br>Data is provided in the form of GeoTIFF files along with GeoJSON files that contain the ground truth annotations. The “condition” property of each feature in the geojson file describes the condition of the building, any other fields in properties can be ignored.</p>
<p>Training data can be obtained from the following Google Sheet:<br><a href="https://docs.google.com/spreadsheets/d/1tP133OqpwvkzHnkmS_3nezpTJMq06tpPM6P2qU6kaZ4/edit?usp=sharing" target="_blank" rel="noopener">https://docs.google.com/spreadsheets/d/1tP133OqpwvkzHnkmS_3nezpTJMq06tpPM6P2qU6kaZ4/edit?usp=sharing</a></p>
<h1 id="道路检测"><a href="#道路检测" class="headerlink" title="道路检测"></a>道路检测</h1><h2 id="1-Massachusetts-Roads-Dataset"><a href="#1-Massachusetts-Roads-Dataset" class="headerlink" title="1. Massachusetts Roads Dataset"></a>1. Massachusetts Roads Dataset</h2><p><a href="https://www.cs.toronto.edu/~vmnih/data/" target="_blank" rel="noopener">https://www.cs.toronto.edu/~vmnih/data/</a><br>The size of all images in these datasets is <strong>1500×1500</strong>, and the resolution is <strong>1m2/pixel</strong>. </p>
<p>The road dataset consists of <strong>1108</strong> sets for training part, <strong>49</strong> for testingpart, and <strong>14</strong> for validation part.<br><strong>Reference:</strong></p>
<ul>
<li><em>Mnih V. Machine learning for aerial image labeling[D]. University of Toronto (Canada), 2013.</em></li>
</ul>
<h2 id="2-SpaceNet-Roads-Dataset"><a href="#2-SpaceNet-Roads-Dataset" class="headerlink" title="2. SpaceNet Roads Dataset"></a>2. SpaceNet Roads Dataset</h2><p><a href="https://spacenetchallenge.github.io/" target="_blank" rel="noopener">https://spacenetchallenge.github.io/</a><br>The Data - Over 8000 Km of roads across the four SpaceNet Areas of Interest.</p>
<table>
<thead>
<tr>
<th style="text-align:center">AOI</th>
<th style="text-align:center">Area of Raster (Sq. Km)</th>
<th style="text-align:center">Road Centerlines (LineString)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">AOI_2_Vegas</td>
<td style="text-align:center">216</td>
<td style="text-align:center">3685 km</td>
</tr>
<tr>
<td style="text-align:center">AOI_3_Paris</td>
<td style="text-align:center">1,030</td>
<td style="text-align:center">425 km</td>
</tr>
<tr>
<td style="text-align:center">AOI_4_Shanghai</td>
<td style="text-align:center">1,000</td>
<td style="text-align:center">3537 km</td>
</tr>
<tr>
<td style="text-align:center">AOI_5_Khartoum</td>
<td style="text-align:center">765</td>
<td style="text-align:center">1030 km</td>
</tr>
</tbody>
</table>
<p>Road Type Breakdown (km of Road)</p>
<table>
<thead>
<tr>
<th style="text-align:center">Road Type</th>
<th style="text-align:center">AOI_2_Vegas</th>
<th style="text-align:center">AOI_3_Paris</th>
<th style="text-align:center">AOI_4_Shanghai</th>
<th style="text-align:center">AOI_5_Khartoum</th>
<th style="text-align:center">Total</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">Motorway</td>
<td style="text-align:center">115</td>
<td style="text-align:center">9</td>
<td style="text-align:center">102</td>
<td style="text-align:center">13</td>
<td style="text-align:center">240</td>
</tr>
<tr>
<td style="text-align:center">Primary</td>
<td style="text-align:center">365</td>
<td style="text-align:center">14</td>
<td style="text-align:center">192</td>
<td style="text-align:center">98</td>
<td style="text-align:center">669</td>
</tr>
<tr>
<td style="text-align:center">Secondary</td>
<td style="text-align:center">417</td>
<td style="text-align:center">58</td>
<td style="text-align:center">501</td>
<td style="text-align:center">66</td>
<td style="text-align:center">1042</td>
</tr>
<tr>
<td style="text-align:center">Tertiary</td>
<td style="text-align:center">3</td>
<td style="text-align:center">11</td>
<td style="text-align:center">34</td>
<td style="text-align:center">68</td>
<td style="text-align:center">115</td>
</tr>
<tr>
<td style="text-align:center">Residential</td>
<td style="text-align:center">1646</td>
<td style="text-align:center">232</td>
<td style="text-align:center">939</td>
<td style="text-align:center">485</td>
<td style="text-align:center">3301</td>
</tr>
<tr>
<td style="text-align:center">Unclassified</td>
<td style="text-align:center">1138</td>
<td style="text-align:center">95</td>
<td style="text-align:center">1751</td>
<td style="text-align:center">165</td>
<td style="text-align:center">3149</td>
</tr>
<tr>
<td style="text-align:center">Cart track</td>
<td style="text-align:center">2</td>
<td style="text-align:center">6</td>
<td style="text-align:center">19</td>
<td style="text-align:center">135</td>
<td style="text-align:center">162</td>
</tr>
<tr>
<td style="text-align:center">Total</td>
<td style="text-align:center">3685</td>
<td style="text-align:center">425</td>
<td style="text-align:center">3537.9</td>
<td style="text-align:center">1030</td>
<td style="text-align:center">8677</td>
</tr>
</tbody>
</table>
<p><strong>Catalog</strong><br>The data is hosted on AWS in a requester pays bucket.<br><figure class="highlight jboss-cli"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">aws s3 <span class="keyword">ls</span> s3:<span class="string">//spacenet-dataset/SpaceNet_Roads_Competition/</span> <span class="params">--request-payer</span> requester</span><br></pre></td></tr></table></figure></p>
<p><strong>Sample Data</strong><br>10 Samples from each AOI - Road Network Extraction Samples<br>To download processed 400mx400m tiles of AOI 2 (728.8 MB) with associated road centerlines for training do the following:<br><figure class="highlight dsconfig"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">aws </span><span class="string">s3api </span><span class="built_in">get-object</span> <span class="built_in">--bucket</span> <span class="string">spacenet-dataset </span><span class="built_in">--key</span> <span class="string">SpaceNet_Roads_Competition/</span><span class="string">SpaceNet_Roads_Sample.</span><span class="string">tar.</span><span class="string">gz </span><span class="built_in">--request-payer</span> <span class="string">requester </span><span class="string">SpaceNet_Roads_Sample.</span><span class="string">tar.</span><span class="string">gz</span></span><br></pre></td></tr></table></figure></p>
<p><strong>Training Data</strong><br>AOI 2 - Vegas - Road Network Extraction Training<br>To download processed 400mx400m tiles of AOI 2 (25 GB) with associated road centerlines for training do the following:<br><figure class="highlight dsconfig"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">aws </span><span class="string">s3api </span><span class="built_in">get-object</span> <span class="built_in">--bucket</span> <span class="string">spacenet-dataset </span><span class="built_in">--key</span> <span class="string">SpaceNet_Roads_Competition/</span><span class="string">AOI_2_Vegas_Roads_Train.</span><span class="string">tar.</span><span class="string">gz </span><span class="built_in">--request-payer</span> <span class="string">requester </span><span class="string">AOI_2_Vegas_Roads_Train.</span><span class="string">tar.</span><span class="string">gz</span></span><br></pre></td></tr></table></figure></p>
<p>AOI 3 - Paris - Road Network Extraction Training<br>To download processed 400mx400m tiles of AOI 3 (5.6 GB) with associated road centerlines for training do the following:<br><figure class="highlight dsconfig"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">aws </span><span class="string">s3api </span><span class="built_in">get-object</span> <span class="built_in">--bucket</span> <span class="string">spacenet-dataset </span><span class="built_in">--key</span> <span class="string">SpaceNet_Roads_Competition/</span><span class="string">AOI_3_Paris_Roads_Train.</span><span class="string">tar.</span><span class="string">gz </span><span class="built_in">--request-payer</span> <span class="string">requester </span><span class="string">AOI_3_Paris_Roads_Train.</span><span class="string">tar.</span><span class="string">gz</span></span><br></pre></td></tr></table></figure></p>
<p>AOI 4 - Shanghai - Road Network Extraction Training<br>To download processed 400mx400m tiles of AOI 4 (25 GB) with associated road centerlines for training do the following:<br><figure class="highlight dsconfig"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">aws </span><span class="string">s3api </span><span class="built_in">get-object</span> <span class="built_in">--bucket</span> <span class="string">spacenet-dataset </span><span class="built_in">--key</span> <span class="string">SpaceNet_Roads_Competition/</span><span class="string">AOI_4_Shanghai_Roads_Train.</span><span class="string">tar.</span><span class="string">gz </span><span class="built_in">--request-payer</span> <span class="string">requester </span><span class="string">AOI_4_Shanghai_Roads_Train.</span><span class="string">tar.</span><span class="string">gz</span></span><br></pre></td></tr></table></figure></p>
<p>AOI 5 - Khartoum - Road Network Extraction Training<br>To download processed 400mx400m tiles of AOI 5 (25 GB) with associated road centerlines for training do the following:<br><figure class="highlight dsconfig"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">aws </span><span class="string">s3api </span><span class="built_in">get-object</span> <span class="built_in">--bucket</span> <span class="string">spacenet-dataset </span><span class="built_in">--key</span> <span class="string">SpaceNet_Roads_Competition/</span><span class="string">AOI_5_Khartoum_Roads_Train.</span><span class="string">tar.</span><span class="string">gz </span><span class="built_in">--request-payer</span> <span class="string">requester </span><span class="string">AOI_5_Khartoum_Roads_Train.</span><span class="string">tar.</span><span class="string">gz</span></span><br></pre></td></tr></table></figure></p>
<p><strong>Test Data</strong><br>AOI 2 - Vegas - Road Network Extraction Testing<br>To download processed 400mx400m tiles of AOI 2 (8.1 GB) for testing do:<br><figure class="highlight dsconfig"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">aws </span><span class="string">s3api </span><span class="built_in">get-object</span> <span class="built_in">--bucket</span> <span class="string">spacenet-dataset </span><span class="built_in">--key</span> <span class="string">SpaceNet_Roads_Competition/</span><span class="string">AOI_2_Vegas_Roads_Test_Public.</span><span class="string">tar.</span><span class="string">gz </span><span class="built_in">--request-payer</span> <span class="string">requester </span><span class="string">AOI_2_Vegas_Roads_Test_Public.</span><span class="string">tar.</span><span class="string">gz</span></span><br></pre></td></tr></table></figure></p>
<p>AOI 3 - Paris - Road Network Extraction Testing<br>To download processed 400mx400m tiles of AOI 3 (1.9 GB) for testing do:<br><figure class="highlight dsconfig"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">aws </span><span class="string">s3api </span><span class="built_in">get-object</span> <span class="built_in">--bucket</span> <span class="string">spacenet-dataset </span><span class="built_in">--key</span> <span class="string">SpaceNet_Roads_Competition/</span><span class="string">AOI_3_Paris_Roads_Test_Public.</span><span class="string">tar.</span><span class="string">gz </span><span class="built_in">--request-payer</span> <span class="string">requester </span><span class="string">AOI_3_Paris_Roads_Test_Public.</span><span class="string">tar.</span><span class="string">gz</span></span><br></pre></td></tr></table></figure></p>
<p>AOI 4 - Shanghai - Road Network Extraction Testing<br>To download processed 400mx400m tiles of AOI 4 (8.1 GB) for testing do:<br><figure class="highlight dsconfig"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">aws </span><span class="string">s3api </span><span class="built_in">get-object</span> <span class="built_in">--bucket</span> <span class="string">spacenet-dataset </span><span class="built_in">--key</span> <span class="string">SpaceNet_Roads_Competition/</span><span class="string">AOI_4_Shanghai_Roads_Test_Public.</span><span class="string">tar.</span><span class="string">gz </span><span class="built_in">--request-payer</span> <span class="string">requester </span><span class="string">AOI_4_Shanghai_Roads_Test_Public.</span><span class="string">tar.</span><span class="string">gz</span></span><br></pre></td></tr></table></figure></p>
<p>AOI 5 - Khartoum - Road Network Extraction Testing<br>To download processed 400mx400m tiles of AOI 5 (8.1 GB) for testing do:<br><figure class="highlight dsconfig"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">aws </span><span class="string">s3api </span><span class="built_in">get-object</span> <span class="built_in">--bucket</span> <span class="string">spacenet-dataset </span><span class="built_in">--key</span> <span class="string">SpaceNet_Roads_Competition/</span><span class="string">AOI_5_Khartoum_Roads_Test_Public.</span><span class="string">tar.</span><span class="string">gz </span><span class="built_in">--request-payer</span> <span class="string">requester </span><span class="string">AOI_5_Khartoum_Roads_Test_Public.</span><span class="string">tar.</span><span class="string">gz</span></span><br></pre></td></tr></table></figure></p>
<h2 id="3-DeepGlobe-Road-Extraction-Challenge"><a href="#3-DeepGlobe-Road-Extraction-Challenge" class="headerlink" title="3. DeepGlobe Road Extraction Challenge"></a>3. DeepGlobe Road Extraction Challenge</h2><p><img src="http://deepglobe.org/uploads/1/1/7/0/117046351/published/pred.png?1518826209" alt=""><br><a href="http://deepglobe.org/index.html" target="_blank" rel="noopener">http://deepglobe.org/index.html</a><br><a href="https://competitions.codalab.org/competitions/18467" target="_blank" rel="noopener">https://competitions.codalab.org/competitions/18467</a><br>In disaster zones, especially in developing countries, maps and accessibility information are crucial for crisis response. We would like to pose the challenge of automatically extracting roads and street networks from satellite images.<br><strong>Reference:</strong></p>
<ul>
<li><em>Demir I, Koperski K, Lindenbaum D, et al. DeepGlobe 2018: A Challenge to Parse the Earth through Satellite Images[J]. arXiv preprint arXiv:1805.06561, 2018.</em></li>
</ul>
<h1 id="变化检测"><a href="#变化检测" class="headerlink" title="变化检测"></a>变化检测</h1><h2 id="1-Onera-Satellite-Change-Detection"><a href="#1-Onera-Satellite-Change-Detection" class="headerlink" title="1. Onera Satellite Change Detection"></a>1. Onera Satellite Change Detection</h2><p><a href="http://dase.ticinumaerospace.com" target="_blank" rel="noopener">http://dase.ticinumaerospace.com</a><br>The Onera Satellite Change Detection (OSCD) dataset address the issue of detecting changes between satellite images at different dates.<br>It comprises <strong>24 pairs of multispectral images</strong> taken from the <strong>Sentinel-2 satellite</strong> in <strong>2015</strong> and <strong>2018</strong>. Locations are picked all over the world, in North and South America, Europe, Middle-East and Asia. For each location, registered pairs of 13-band multispectral satellite images obtained by the Sentinel-2 satellites are provided. Images vary in spatial resolution between 10m, 20m and 60m.</p>
<p>Pixel-level change groundtruth is provided for <strong>14</strong> of the image pairs. The annotated changes focus on urban changes, such as new buildings or new roads. These data can be used for training and setting parameters of change detection algorithms.<br><img src="https://www.onera.fr/sites/default/files/300/beirut-triptych.png" alt=""><br><strong>References:</strong><br><em>Urban Change Detection for Multispectral Earth Observation Using Convolutional Neural Networks R. Caye Daudt, B. Le Saux, A. Boulch, Y. Gousseau IEEE International Geoscience and Remote Sensing Symposium (IGARSS’2018) Valencia, Spain, July 2018</em></p>
<h1 id="其他数据集"><a href="#其他数据集" class="headerlink" title="其他数据集"></a>其他数据集</h1><h2 id="RSICD"><a href="#RSICD" class="headerlink" title="RSICD"></a>RSICD</h2><p><a href="https://github.com/201528014227051/RSICD_optimal" target="_blank" rel="noopener">https://github.com/201528014227051/RSICD_optimal</a><br><img src="https://github.com/201528014227051/RSICD_optimal/raw/master/example.PNG" alt=""><br>RSICD is used for remote sensing <strong>image captioning</strong> task. more than ten thousands remote sensing images are collected from Google Earth, Baidu Map, MapABC, Tianditu. The images are fixed to <strong>224X224</strong> pixels with various resolutions. The total number of remote sensing images are <strong>10921</strong>, with <strong>five sentences descriptions per image</strong>. To the best of our knowledge, this dataset is the largest dataset for remote sensing captioning. The sample images in the dataset are with high intra-class diversity and low inter-class dissimilarity. Thus, this dataset provides the researchers a data resource to advance the task of remote sensing captioning.<br><strong>Reference:</strong></p>
<ul>
<li><em>Lu X, Wang B, Zheng X, et al. Exploring Models and Data for Remote Sensing Image Caption Generation[J]. IEEE Transactions on Geoscience and Remote Sensing, 2017.</em></li>
</ul>
<h2 id="The-IARPA-Functional-Map-of-the-World-fMoW-Challenge"><a href="#The-IARPA-Functional-Map-of-the-World-fMoW-Challenge" class="headerlink" title="The IARPA Functional Map of the World (fMoW) Challenge"></a>The IARPA Functional Map of the World (fMoW) Challenge</h2><p><a href="https://www.iarpa.gov/challenges/fmow.html" target="_blank" rel="noopener">https://www.iarpa.gov/challenges/fmow.html</a><br><a href="https://spacenetchallenge.github.io/datasets/fmow_summary.html" target="_blank" rel="noopener">https://spacenetchallenge.github.io/datasets/fmow_summary.html</a><br><a href="https://github.com/fmow" target="_blank" rel="noopener">https://github.com/fmow</a><br>Intelligence analysts, policy makers, and first responders around the world rely on geospatial land use data to inform crucial decisions about global defense and humanitarian activities. Historically, analysts have manually identified and classified geospatial information by comparing and analyzing satellite images, but that process is time consuming and insufficient to support disaster response. The fMoW Challenge sought to foster breakthroughs in the automated analysis of overhead imagery by harnessing the collective power of the global data science and machine learning communities; empowering stakeholders to bolster their capabilities through computer vision automation. The challenge published one of the largest publicly available satellite-image datasets to date, with more than one million points of interest from around the world. The dataset also contains other elements such as temporal views, multispectral imagery, and satellite-specific metadata that researchers can exploit to build novel algorithms capable of classifying facility, building, and land use.<br><strong>Reference:</strong></p>
<ul>
<li><em>Gordon Christie, Neil Fendley, James Wilson, Ryan Mukherjee; The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018, pp. 6172-6180</em></li>
</ul>
<h2 id="IARPA-Multi-View-Stereo-3D-Mapping-Challenge"><a href="#IARPA-Multi-View-Stereo-3D-Mapping-Challenge" class="headerlink" title="IARPA Multi-View Stereo 3D Mapping Challenge"></a>IARPA Multi-View Stereo 3D Mapping Challenge</h2><p><a href="https://www.iarpa.gov/challenges/3dchallenge.html" target="_blank" rel="noopener">https://www.iarpa.gov/challenges/3dchallenge.html</a><br><a href="https://spacenetchallenge.github.io/datasets/mvs_summary.html" target="_blank" rel="noopener">https://spacenetchallenge.github.io/datasets/mvs_summary.html</a><br>This data set includes DigitalGlobe <strong>WorldView-3 panchromatic and multispectral images</strong> of a <strong>100 square kilometer area</strong> near San Fernando, Argentina. We also provide <strong>20cm airborne lidar ground truth data</strong> for a <strong>20 square kilometer</strong> subset of this area and performance analysis software to assess accuracy and completeness metrics. Commercial satellite imagery is provided courtesy of DigitalGlobe, and ground truth lidar is provided courtesy of IARPA.<br><strong>Catalog</strong><br><figure class="highlight armasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="symbol">aws</span> <span class="built_in">s3</span> ls <span class="built_in">s3</span>://spacenet-dataset/mvs_dataset</span><br></pre></td></tr></table></figure></p>
<p>The catalog contains the following packages:</p>
<ul>
<li>Updated metric analysis software with examples from contest winners</li>
<li>Challenge data package with instructions, cropped TIFF images, ground truth, image cropping software, and metric scoring software (1.2 GB)</li>
<li>JHU/APL example MVS solution (451 MB)</li>
<li>NITF panchromatic, multispectral, and short-wave infrared DigitalGlobe WorldView-3 satellite images (72.1 GB)</li>
<li>LAZ lidar point clouds with SBET (2.2 GB)</li>
<li>Spectral image calibration software (84 MB)</li>
</ul>
<h2 id="SEN1-2"><a href="#SEN1-2" class="headerlink" title="SEN1-2"></a>SEN1-2</h2><p><a href="https://mediatum.ub.tum.de/1436631" target="_blank" rel="noopener">https://mediatum.ub.tum.de/1436631</a><br>SEN1-2 is a dataset consisting of 282,384 pairs of corresponding synthetic aperture radar and optical image patches acquired by the Sentinel-1 and Sentinel-2 remote sensing satellites, respectively. The SEN1-2 dataset is published to foster deep learning research in SAR-optical data fusion. Several possible applications, such as SAR image colorization, SAR-optical image matching, and creation of artificial optical images from SAR input data.<br>Download： <a href="http://138.246.224.34/index.php/s/m1436631" target="_blank" rel="noopener">http://138.246.224.34/index.php/s/m1436631</a><br>The data server also offers downloads with <a href="ftp://138.246.224.34/" target="_blank" rel="noopener">FTP</a><br>The data server also offers downloads with rsync (password m1436631):<br>rsync rsync:<a href="mailto://m1436631@138.246.224.34" target="_blank" rel="noopener">//m1436631@138.246.224.34</a>/m1436631/ </p>
<h2 id="EuroSDR航空影像密集匹配数据"><a href="#EuroSDR航空影像密集匹配数据" class="headerlink" title="EuroSDR航空影像密集匹配数据"></a>EuroSDR航空影像密集匹配数据</h2><p><a href="http://www.ifp.uni-stuttgart.de/ISPRS-EuroSDR/ImageMatching/index.en.html" target="_blank" rel="noopener">http://www.ifp.uni-stuttgart.de/ISPRS-EuroSDR/ImageMatching/index.en.html</a><br>鉴于自动影像匹配软件的不断发展，该测试数据集旨在评估摄影测量三维数据采集的潜力。试验数据包括两组垂直摄影和一组倾斜摄影。倾斜数据的覆盖区域是瑞士苏黎世，地面分辨率为6~13 cm，垂直影像的覆盖区域分别是德国Vaihingen(地面分辨率为20 cm)和慕尼黑(地面分辨率为10 cm)。</p>
<h2 id="Okutama-Action"><a href="#Okutama-Action" class="headerlink" title="Okutama-Action"></a>Okutama-Action</h2><p><a href="http://okutama-action.org/" target="_blank" rel="noopener">http://okutama-action.org/</a><br>an aerial view concurrent human action detection dataset. It contains 43 fully-annotated sequences of 12 human action classes. They used two UAV pilots to capture 22 different scenarios using 9 participants from a distance of 10-45 meters and camera angles of 45-90. The dataset is in the form of 30 FPS videos that provides 77365 images. They splitted the dataset into the training (33 videos) and testing samples (10 videos). </p>
<h2 id="VEDAI-Vehicle-Dataset"><a href="#VEDAI-Vehicle-Dataset" class="headerlink" title="VEDAI Vehicle Dataset"></a>VEDAI Vehicle Dataset</h2><p><a href="https://downloads.greyc.fr/vedai/" target="_blank" rel="noopener">https://downloads.greyc.fr/vedai/</a><br>VEDAI is a dataset for Vehicle Detection in Aerial Imagery, provided as a tool to benchmark automatic target recognition algorithms in unconstrained environments. The vehicles contained in the database, in addition of being small, exhibit different variabilities such as multiple orientations, lighting/shadowing changes, specularities or occlusions. Furthermore, each image is available in several spectral bands and resolutions. A precise experimental protocol is also given, ensuring that the experimental results obtained by different people can be properly reproduced and compared. We also give the performance of some baseline algorithms on this dataset, for different settings of these algorithms, to illustrate the difficulties of the task and provide baseline comparisons.</p>
<p><strong>Reference:</strong></p>
<ul>
<li><em>Vehicle Detection in Aerial Imagery: A small target detection benchmark., Sébastien Razakarivony and Frédéric Jurie, Journal of Visual Communication and Image Representation, 2015.</em></li>
</ul>
<h2 id="MAFAT-Challenge-Fine-Grained-Classification-of-Objects-from-Aerial-Imagery"><a href="#MAFAT-Challenge-Fine-Grained-Classification-of-Objects-from-Aerial-Imagery" class="headerlink" title="MAFAT Challenge - Fine-Grained Classification of Objects from Aerial Imagery"></a>MAFAT Challenge - Fine-Grained Classification of Objects from Aerial Imagery</h2><p><a href="http://mafatchallenge.mod.gov.il/" target="_blank" rel="noopener">http://mafatchallenge.mod.gov.il/</a><br><a href="https://competitions.codalab.org/competitions/19854" target="_blank" rel="noopener">https://competitions.codalab.org/competitions/19854</a><br><a href="https://groups.google.com/a/mafatchallenge.com/forum/?hl=en#!forum/mafat-challenge-forum" target="_blank" rel="noopener">https://groups.google.com/a/mafatchallenge.com/forum/?hl=en#!forum/mafat-challenge-forum</a></p>
<h3 id="Data"><a href="#Data" class="headerlink" title="Data"></a>Data</h3><p>The dataset consists of aerial imagery taken from diverse geographical locations, different times, resolutions, area coverage and image acquisition conditions (weather, sun direction, camera direction, etc). Image resolution varies between 5cm to 15cm GSD (Ground Sample Distance).</p>
<p>Few examples are presented below:<br><img src="https://s3-us-west-2.amazonaws.com/codalab-webiks/Images/examples.jpg" alt=""></p>
<h3 id="Task-Specifications"><a href="#Task-Specifications" class="headerlink" title="Task Specifications"></a>Task Specifications</h3><p>Participants are asked to classify objects in four granularity levels:</p>
<ul>
<li>Class - every object is categorized into one of the following major classes: ‘Large Vehicles’ or ‘Small Vehicles’.</li>
<li>Subclass - objects are categorized to subclasses according to their function or designation, for example: Cement mixer, Crane truck, Prime mover, etc. Each object should be assigned to a single subclass.</li>
<li>Presence of features - objects are labeled according to their characteristics. For example: has a Ladder? is Wrecked? has a Sunroof? etc. Each object may be labeled with multiple different features</li>
<li>Object perceived color - Objects are labeled with their (human) percieved color.  For example: Blue, Red, Yellow etc. Each object includes a single color value.</li>
</ul>
<p>Here is a full description of the competition dataset’s tagging hierarchy:</p>
<ul>
<li>Small vehicle<ol>
<li>Subclasses - Sedan, Hatchback, Minivan, Van, Pickup truck, Jeep, Public vehicle.</li>
<li>Features - Sunroof, Luggage carrier, Open cargo area, Enclosed cab, Wrecked, Spare wheel.</li>
<li>Colors - Yellow, Red, Blue, Black, Silver/Grey, White, Other.</li>
</ol>
</li>
<li>Large vehicle<ol>
<li>Subclasses - Truck, Light truck, Cement mixer, Dedicated agricultural vehicle, Crane truck, Prime mover, Tanker, Bus, Minibus.</li>
<li>Features - Open cargo area, AC vents, Wrecked, Enclosed box, Enclosed cab, Ladder, Flatbed, Soft shell box, Harnessed to a cart.</li>
<li>Colors - Yellow, Red, Blue, Black, Silver/Grey, White, Other.</li>
</ol>
</li>
</ul>
<h3 id="Training-Set"><a href="#Training-Set" class="headerlink" title="Training Set"></a>Training Set</h3><p>Participants will receive a training set, consisting of</p>
<ul>
<li>training imagery - a folder containing 1,663 tiff and jpeg images.</li>
<li>train.csv - A CSV file contining 11,617 tagged vehicles. Each object is represented by a tag ID, an ID of the image in which it is located, and a bounding polygon, which is a set of 4 x-y (pixel) coordinates. Additionally, each object in the training set includes fine-grained classification data: class, subclass, features and color. Please note that features are represented as boolean fields while “-1” represent a non-viable option. </li>
</ul>
<h3 id="Test-Set"><a href="#Test-Set" class="headerlink" title="Test Set"></a>Test Set</h3><p>Participants will also receive a test set, consisting of</p>
<ul>
<li>test imagery - a folder containing 2,553 tiff and jpeg images.</li>
<li>test.csv - A CSV file containing 11,879 tagged vehicles. This file includes objects in the same form as in the training set (tag ID, image ID and a bounding polygon), but without the classification data.</li>
</ul>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2018/06/11/Context-Encoding-for-Semantic-Segmentation/" rel="next" title="论文阅读 - Context Encoding for Semantic Segmentation<br>(CVPR2018)">
                <i class="fa fa-chevron-left"></i> 论文阅读 - Context Encoding for Semantic Segmentation<br>(CVPR2018)
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2018/06/14/Vortex-Pooling-Improving-Context-Representation-in-Semantic-Segmentation/" rel="prev" title="论文阅读 - Vortex Pooling: Improving Context Representation in Semantic Segmentation">
                论文阅读 - Vortex Pooling: Improving Context Representation in Semantic Segmentation <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            Overview
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image"
                src="/images/jonathon-reed-190894.jpg"
                alt="Bin Zhang" />
            
              <p class="site-author-name" itemprop="name">Bin Zhang</p>
              <p class="site-description motion-element" itemprop="description">I am currently a Master candidate in Wuhan University majoring Photogrammetry and remote sensing</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives">
              
                  <span class="site-state-item-count">20</span>
                  <span class="site-state-item-name">posts</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">4</span>
                  <span class="site-state-item-name">categories</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">20</span>
                  <span class="site-state-item-name">tags</span>
                </a>
              </div>
            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#场景分类"><span class="nav-text">场景分类</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#1-UC-Merced-Land-Use-Dataset"><span class="nav-text">1. UC Merced Land Use Dataset</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-WHU-RS19"><span class="nav-text">2. WHU-RS19</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-RSSCN7"><span class="nav-text">3. RSSCN7</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-SAT-4-and-SAT-6-airborne-datasets"><span class="nav-text">4. SAT-4 and SAT-6 airborne datasets</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#5-RSC11"><span class="nav-text">5. RSC11</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#6-SIRI-WHU"><span class="nav-text">6. SIRI-WHU</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#7-AID"><span class="nav-text">7. AID</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#8-NWPU-RESISC45"><span class="nav-text">8. NWPU-RESISC45</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#9-PatternNet"><span class="nav-text">9. PatternNet</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#10-RSI-CB"><span class="nav-text">10. RSI-CB</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#11-AID"><span class="nav-text">11. AID++</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#目标检测"><span class="nav-text">目标检测</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#1-RSOD-Dataset"><span class="nav-text">1. RSOD-Dataset</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-NWPU-VHR-10-dataset"><span class="nav-text">2. NWPU VHR-10 dataset</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-Cars-Overhead-With-Context-COWC"><span class="nav-text">3. Cars Overhead With Context(COWC)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-DOTA"><span class="nav-text">4. DOTA</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#5-DIUx-xView-2018-Detection-Challenge"><span class="nav-text">5. DIUx xView 2018 Detection Challenge</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#语义分割"><span class="nav-text">语义分割</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#1-Zurich-Summer-Dataset"><span class="nav-text">1. Zurich Summer Dataset</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-ISPRS-Test-Project-on-Urban-Classification-and-3D-Building-Reconstruction–2D-Semantic-Labeling-Contest"><span class="nav-text">2. ISPRS Test Project on Urban Classification and 3D Building Reconstruction–2D Semantic Labeling Contest</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Vaihingen"><span class="nav-text">Vaihingen</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Potsdam"><span class="nav-text">Potsdam</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-2017-IEEE-GRSS-Data-Fusion-Contest"><span class="nav-text">3. 2017 IEEE GRSS Data Fusion Contest</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-2018-IEEE-GRSS-Data-Fusion-Contest–Advanced-multi-sensor-optical-remote-sensing-for-urban-land-use-and-land-cover-classification"><span class="nav-text">4. 2018 IEEE GRSS Data Fusion Contest–Advanced multi-sensor optical remote sensing for urban land use and land cover classification</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#5-EvLab-SS-Dataset"><span class="nav-text">5. EvLab-SS Dataset</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#6-DeepGlobe-Land-Cover-Classification-Challenge"><span class="nav-text">6. DeepGlobe Land Cover Classification Challenge</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#7-Gaofen-Image-Dataset-GID"><span class="nav-text">7. Gaofen Image Dataset (GID)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#8-Airbus-Ship-Detection-Challenge"><span class="nav-text">8. Airbus Ship Detection Challenge</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#建筑物检测"><span class="nav-text">建筑物检测</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#1-Massachusetts-Buildings-Dataset"><span class="nav-text">1. Massachusetts Buildings Dataset</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-SpaceNet-Buildings-Dataset"><span class="nav-text">2. SpaceNet Buildings Dataset</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#SpaceNet-Buildings-Dataset-Round-1"><span class="nav-text">SpaceNet Buildings Dataset Round 1</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#SpaceNet-Buildings-Dataset-Round-2"><span class="nav-text">SpaceNet Buildings Dataset Round 2</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-Inria-Aerial-Image-Labeling-Dataset"><span class="nav-text">3. Inria Aerial Image Labeling Dataset</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-The-USSOCOM-Urban-3D-Challenge"><span class="nav-text">4. The USSOCOM Urban 3D Challenge</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#5-DeepGlobe-Building-Extraction-Challenge"><span class="nav-text">5. DeepGlobe Building Extraction Challenge</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#6-CrowdAI-Mapping-Challenge"><span class="nav-text">6. CrowdAI Mapping Challenge</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#7-WHU-Building-Dataset"><span class="nav-text">7. WHU Building Dataset</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#8-AIRS-Aerial-Imagery-for-Roof-Segmentation"><span class="nav-text">8. AIRS (Aerial Imagery for Roof Segmentation)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#9-2018-Open-AI-Tanzania-Building-Footprint-Segmentation-Challenge"><span class="nav-text">9. 2018 Open AI Tanzania Building Footprint Segmentation Challenge</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#道路检测"><span class="nav-text">道路检测</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#1-Massachusetts-Roads-Dataset"><span class="nav-text">1. Massachusetts Roads Dataset</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-SpaceNet-Roads-Dataset"><span class="nav-text">2. SpaceNet Roads Dataset</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-DeepGlobe-Road-Extraction-Challenge"><span class="nav-text">3. DeepGlobe Road Extraction Challenge</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#变化检测"><span class="nav-text">变化检测</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#1-Onera-Satellite-Change-Detection"><span class="nav-text">1. Onera Satellite Change Detection</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#其他数据集"><span class="nav-text">其他数据集</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#RSICD"><span class="nav-text">RSICD</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#The-IARPA-Functional-Map-of-the-World-fMoW-Challenge"><span class="nav-text">The IARPA Functional Map of the World (fMoW) Challenge</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#IARPA-Multi-View-Stereo-3D-Mapping-Challenge"><span class="nav-text">IARPA Multi-View Stereo 3D Mapping Challenge</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#SEN1-2"><span class="nav-text">SEN1-2</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#EuroSDR航空影像密集匹配数据"><span class="nav-text">EuroSDR航空影像密集匹配数据</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Okutama-Action"><span class="nav-text">Okutama-Action</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#VEDAI-Vehicle-Dataset"><span class="nav-text">VEDAI Vehicle Dataset</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#MAFAT-Challenge-Fine-Grained-Classification-of-Objects-from-Aerial-Imagery"><span class="nav-text">MAFAT Challenge - Fine-Grained Classification of Objects from Aerial Imagery</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Data"><span class="nav-text">Data</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Task-Specifications"><span class="nav-text">Task Specifications</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Training-Set"><span class="nav-text">Training Set</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Test-Set"><span class="nav-text">Test Set</span></a></li></ol></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      
        <div class="back-to-top">
          <i class="fa fa-arrow-up"></i>
          
            <span id="scrollpercent"><span>0</span>%</span>
          
        </div>
      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2018</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Bin Zhang</span>

  
</div>


  <div class="powered-by">Powered by <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a></div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Gemini</a> v5.1.4</div>




        







        
      </div>
    </footer>

    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  


  











  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  

  
  
    <script type="text/javascript" src="/lib/canvas-nest/canvas-nest.min.js"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  

  

  
  

  
  


  

  

</body>
</html>
